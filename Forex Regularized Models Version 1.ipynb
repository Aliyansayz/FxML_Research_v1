{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c3d978-02d1-4913-9be7-ea724613576a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8586ccaf-5179-4848-bb6e-debce8c0b5b0",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://raw.githubusercontent.com/Aliyansayz/FxML_Research_v1/main/images/primary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(1).png\" target=\"_blank\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Aliyansayz/FxML_Research_v1/main/images/primary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(1).png\" width=\"600\" alt=\"Fx ML Research v1\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8cb36-0619-43de-bfe1-435a87a2a826",
   "metadata": {
    "tags": []
   },
   "source": [
    "Repository Link : \n",
    "    https://github.com/Aliyansayz/FxML_Research_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365ebf6-f6c0-48d1-8915-d8d1ef498303",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Forex Models With Regularized Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3767ae7c-5f70-422a-9e0d-e902f423c8c4",
   "metadata": {},
   "source": [
    "## Version 1 \n",
    "* Temporal features True (Month, Day of week, Week Of Month)\n",
    "* Hour 4 Features Lag by 7\n",
    "* Day Features (All included) Lag by 9\n",
    "* Starting Date June 2020 Training Till September 2023\n",
    "* Ending Date May 2024\n",
    "* Hour 4 Features 'relative range', 'candle type', 'heikin ashi' \n",
    "* Hyperparameters Regularized\n",
    "\n",
    "\n",
    "Version 1 Regularized On Two Months April May On Two Conditions :\n",
    " \n",
    " 1- hyper parameters chosen should give positive gains for any currency pairs for those two months\n",
    " \n",
    " 2- Previous months accuracy should be 50% and net gains accuracy close to 50% or 52%\n",
    "\n",
    "Results :- (pips)\n",
    "* ‚úÖOctober 2167\n",
    "* ‚úÖ November 2498\n",
    "* ‚ùå December -3080\n",
    "* ‚úÖ January 4844\n",
    "* ‚úÖ February 1288\n",
    "* ‚úÖ March 1883\n",
    "\n",
    "üìâ Not all models were in positive gains in previous month despite all performing good on april may with positive gains\n",
    "\n",
    "‚≠ê But parameters performing good on two different months gave them ability to keep majority of currency pairs model in positive gain\n",
    "\n",
    "üí¢ December is also not recommended for trading by trading community so we might ignore that month outcomes \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10584b4-5c8e-420d-86f5-07b937a839da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results_dict = {}\n",
    "day_features, hour4_features = load_features_files()\n",
    "forex_pairs = [\n",
    "    'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY'\n",
    "        ]\n",
    "\n",
    "symbol_hyperparameter = [\n",
    " 'AUDJPY', 'AUDUSD', 'AUDCAD', 'AUDCHF', 'AUDNZD',\n",
    " 'CADJPY', \n",
    " 'EURAUD', 'EURCAD', 'EURUSD', 'EURGBP', 'EURNZD',\n",
    " 'GBPCAD', 'GBPCHF', 'GBPUSD', 'GBPNZD',\n",
    " 'NZDCHF', 'NZDJPY',\n",
    " 'USDCHF', 'USDJPY', 'USDCAD',\n",
    " 'CADCHF', \n",
    " 'NZDCAD', 'NZDUSD']\n",
    "\n",
    "# symbols = ['USDCAD']\n",
    "\n",
    "for symbol in forex_pairs:\n",
    "    \n",
    "    \n",
    "    day_data, hour4_data = get_day_hour4_features(symbol, day_features, hour4_features)\n",
    "\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = get_features_transformed(symbol, day_data, hour4_data)\n",
    "    \n",
    "    print(symbol)\n",
    "    # model = load_model(symbol)\n",
    "    \n",
    "    if symbol in symbol_hyperparameter: \n",
    "      \n",
    "       symbol_parameters = load_parameters(symbol)\n",
    "       iteration, learning_rate, depth = symbol_parameters['Iterations'], symbol_parameters['Lr'], symbol_parameters['depth']\n",
    "       parameters = [ iteration, learning_rate, depth ] \n",
    "       model = train_model(X_train_scaled, X_test_scaled, y_train, y_test, parameters=parameters)\n",
    "    \n",
    "    else:\n",
    "      model = train_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "    # model = load_model(symbol)\n",
    "    gains , accuracy = evaluate_model_return_gains_accuracy(symbol, model, X_test_scaled,  y_test)\n",
    "    \n",
    "     # 2  # 1 --> May 2024 , # 2 --> April 2024\n",
    "    step = 1 # ____          May \n",
    "    # step = 2 # ____       April\n",
    "    cluster = 22 # possible days of trading in a year data ends at 31 May\n",
    "    gains, accuracy = evaluate_model_return_gains_accuracy(symbol, model, X_test_scaled, y_test, custom_sample= [step, cluster] ) # custom_sample= None, last_cluster = None  \n",
    "    net_gains = gains['net_gains']\n",
    "    \n",
    "    accuracy_by_net_gains = gains['accuracy_by_net_gains']\n",
    "    results_dict[symbol] = { 'net_gains': int(net_gains), 'accuracy': int(accuracy), 'month': 'may', \n",
    "                             'accuracy_by_net_gains': int(accuracy_by_net_gains)  }\n",
    "    save_model(symbol, model)\n",
    "    \n",
    "\n",
    "    # accuracy_by_net_gains = gains['accuracy_by_net_gains_before']\n",
    "    accuracy_by_net_gains_before = gains['accuracy_by_net_gains_before']\n",
    "\n",
    "    print('net profit ', net_gains)\n",
    "    # print('accuracy_by_net_gains \\n',accuracy_by_net_gains )\n",
    "    print('\\naccuracy by net gains on previous test data ', accuracy_by_net_gains_before)\n",
    "    print(\"accuracy on all test points excluding last 22 points \", accuracy )\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb624f9b-8aee-47d1-b74a-255d3fd57b90",
   "metadata": {},
   "source": [
    "\n",
    "### Results Comparison On Jan Feb Mar For Optimized Hyperparameters On \n",
    "* Two Months Vs \n",
    "* Single Month Vs \n",
    "* Mon Optimized Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f5002-3e59-43ba-871c-a122981a4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results_dict = {}\n",
    "day_features, hour4_features = load_features_files()\n",
    "forex_pairs = [\n",
    "    'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY'\n",
    "        ]\n",
    "\n",
    "symbol_hyperparameter = [\n",
    " 'AUDJPY', 'AUDUSD', 'AUDCAD', 'AUDCHF', 'AUDNZD',\n",
    " 'CADJPY', \n",
    " 'EURAUD', 'EURCAD', 'EURUSD', 'EURGBP', 'EURNZD',\n",
    " 'GBPCAD', 'GBPCHF', 'GBPUSD', 'GBPNZD',\n",
    " 'NZDCHF', 'NZDJPY',\n",
    " 'USDCHF', 'USDJPY', 'USDCAD',\n",
    " 'CADCHF', \n",
    " 'NZDCAD', 'NZDUSD']\n",
    "\n",
    "# symbols = ['USDCAD']\n",
    "\n",
    "for symbol in forex_pairs:\n",
    "    \n",
    "    \n",
    "    day_data, hour4_data = get_day_hour4_features(symbol, day_features, hour4_features)\n",
    "\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = get_features_transformed(symbol, day_data, hour4_data)\n",
    "    \n",
    "    print(symbol)\n",
    "    # model = load_model(symbol, path = f\"forex_models_custom_hyperparam/{symbol}_model_pips_change\")\n",
    "    # model = load_model(symbol, path =f'forex_models_low_lr_few_h4features_300 iteration/{symbol}_model_pips_change')\n",
    "    # model = load_model(symbol, path = f\"forex_models_regularized\")\n",
    "    model = load_model(symbol, path = f\"forex_models_regularized_h4_lag_3\")\n",
    "    \n",
    "    \n",
    "    #   \n",
    "\n",
    "    # if symbol in symbol_hyperparameter: \n",
    "      \n",
    "#        symbol_parameters = load_parameters(symbol)\n",
    "#        iteration, learning_rate, depth = symbol_parameters['Iterations'], symbol_parameters['Lr'], symbol_parameters['depth']\n",
    "#        parameters = [ iteration, learning_rate, depth ] \n",
    "#        model = train_model(X_train_scaled, X_test_scaled, y_train, y_test, parameters=parameters)\n",
    "    \n",
    "#     else:\n",
    "#       model = train_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "    # model = load_model(symbol)\n",
    "    gains , accuracy = evaluate_model_return_gains_accuracy(symbol, model, X_test_scaled,  y_test)\n",
    "    \n",
    "     # 2  # 1 --> May 2024 , # 2 --> April 2024\n",
    "    # step = 1 # ____          May \n",
    "    # step = 5 \n",
    "    \n",
    "    # step = 4 # February\n",
    "\n",
    "    # step = 5 # January\n",
    "\n",
    "    step = 6 # December\n",
    "    # step = 7 # November\n",
    "    # step = 8 # October\n",
    "    \n",
    "    month_key = {'8': 'oct', '7': 'nov', '6':'dec', '5': 'jan', '4':'feb', '3':'mar', '2':'april', '1':'may' }\n",
    "    month_name = 'may' if step == 1 else 'april'\n",
    "    \n",
    "    try: month_name = month_key[str(step)]\n",
    "    except : pass\n",
    "    \n",
    "    cluster = 22 # possible days of trading in a year data ends at 31 May\n",
    "    gains, accuracy = evaluate_model_return_gains_accuracy(symbol, model, X_test_scaled, y_test, custom_sample= [step, cluster] ) # custom_sample= None, last_cluster = None  \n",
    "    \n",
    "    \n",
    "    if step == 3 : month_name = 'march' \n",
    "    if step == 4 : month_name = 'feb'\n",
    "    if step == 5 : month_name = 'jan'\n",
    "    net_gains = gains['net_gains']\n",
    "    accuracy_by_net_gains = gains['accuracy_by_net_gains']\n",
    "    # results_dict[symbol] = { 'net_gains': int(net_gains), 'accuracy': int(accuracy), 'month': month_name, \n",
    "    #                          'accuracy_by_net_gains': int(accuracy_by_net_gains)  }\n",
    "\n",
    "    net_gains = gains['net_gains']\n",
    "    \n",
    "    accuracy_by_net_gains = gains['accuracy_by_net_gains']\n",
    "    results_dict[symbol] = { 'net_gains': int(net_gains), 'accuracy': int(accuracy), 'month': month_name, \n",
    "                             'accuracy_by_net_gains': int(accuracy_by_net_gains)  }\n",
    "    # save_model(symbol, model)\n",
    "    \n",
    "    # accuracy_by_net_gains = gains['accuracy_by_net_gains_before']\n",
    "    accuracy_by_net_gains_before = gains['accuracy_by_net_gains_before']\n",
    "\n",
    "    print('net profit ', net_gains)\n",
    "    # print('accuracy_by_net_gains \\n',accuracy_by_net_gains )\n",
    "    print('\\naccuracy by net gains on previous test data ', accuracy_by_net_gains_before)\n",
    "    print(\"accuracy on all test points excluding last 22 points \", accuracy )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44e9f4-65cd-48ae-b9aa-7a97a56e15e4",
   "metadata": {},
   "source": [
    "## Version 1 \n",
    "* Temporal features True (Month, Day of week, Week Of Month)\n",
    "* Hour 4 Features Lag by 7\n",
    "* Day Features (All included) Lag by 9\n",
    "* Starting Date June 2020 Training Till September 2023\n",
    "* Ending Date May 2024\n",
    "* Hour 4 Features 'relative range', 'candle type', 'heikin ashi' \n",
    "* Hyperparameters Regularized\n",
    "\n",
    "\n",
    "Version 1 Regularized On Two Months April May On Two Conditions :\n",
    " \n",
    " 1- hyper parameters chosen should give positive gains for any currency pairs for those two months\n",
    " \n",
    " 2- Previous months accuracy should be 50% and net gains accuracy close to 50% or 52%\n",
    "\n",
    "Results :- (pips)\n",
    "* ‚úÖOctober 2167\n",
    "* ‚úÖ November 2498\n",
    "* ‚ùå December -3080\n",
    "* ‚úÖ January 4844\n",
    "* ‚úÖ February 1288\n",
    "* ‚úÖ March 1883\n",
    "\n",
    "üìâ Not all models were in positive gains in previous month despite all performing good on april may with positive gains\n",
    "\n",
    "‚≠ê But parameters performing good on two different months gave them ability to keep majority of currency pairs model in positive gain\n",
    "\n",
    "üí¢ December is also not recommended for trading by trading community so we might ignore that month outcomes \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d890b2b-6cdf-4f86-b862-28a4183811d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e01a6-8b84-4995-b4de-3825a46eedeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54fd1b-83a4-4dff-a8c7-ecb2633d2521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cfab5-2090-49dc-97c0-7b1d71e3bbd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For December On Apri-May Month Regularized Data\n",
    "# print(results_dict)\n",
    "\n",
    "\n",
    "# Calculate the final sum of net gains\n",
    "final_sum_net_gain = sum(info['net_gains'] for info in results_dict.values())\n",
    "\n",
    "# print(\"Final sum of net gains December: \", final_sum_net_gain)\n",
    "\n",
    "\n",
    "# Create a list of tuples (symbol, net_gain) and sort it by net_gain in descending order\n",
    "sorted_list = sorted(results_dict.items(), key=lambda x: x[1]['net_gains'], reverse=False) \n",
    "# reverse = False ascending order\n",
    "# reverse = True descending order\n",
    "\n",
    "\n",
    "# Convert the sorted list back to a dictionary if needed\n",
    "sorted_dict = {symbol: info for symbol, info in sorted_list}\n",
    "\n",
    "# final_sum_net_gain = (  dict(symbol, symbol['pips_loss']) for symbol in pl_info.values())\n",
    "\n",
    "loss_list = [ (symbol, value['net_gains'])  for symbol, value in sorted_dict.items() ]\n",
    "\n",
    "\n",
    "print(\"Most Losses In December: \", loss_list )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7f948-1438-488a-996b-1f72505d2765",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#\n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(2).png?raw=true\"_blank\">\n",
    "    <img src=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(2).png?raw=true\" width=\"600\" alt=\"Fx ML Research v1\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(3).png?raw=true\" target=\"_blank\">\n",
    "    <img src=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(3).png?raw=true\" width=\"600\" alt=\"Fx ML Research v1\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(4).png?raw=true\" target=\"_blank\">\n",
    "    <img src=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(4).png?raw=true\" width=\"600\" alt=\"Fx ML Research v1\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(5).png?raw=true\" target=\"_blank\">\n",
    "    <img src=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/secondary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach(5).png?raw=true\" width=\"600\" alt=\"Fx ML Research v1\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be510e95-e455-4103-b559-c628c0c5dbb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hyperparameters Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c97177-788d-4d92-8e6f-8705e0fe78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    'default':{\n",
    "        'Iterations': 300,\n",
    "        'Lr': 0.01,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 6\n",
    "    },\n",
    "    \n",
    "    'NZDJPY': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.001,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 6\n",
    "    },\n",
    "    'AUDJPY': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.01,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 6\n",
    "    },\n",
    "    'USDJPY': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.01,\n",
    "        'depth': 6\n",
    "    },\n",
    "    'CADJPY': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.01,\n",
    "        'depth': 6\n",
    "    },\n",
    "    'NZDCAD': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 114\n",
    "    },\n",
    "    'CADCHF': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 158\n",
    "    },\n",
    "    'USDCHF2': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 50,\n",
    "        'net pips profit': 179\n",
    "    },\n",
    "    'EURAUD': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 56,\n",
    "        'net pips profit': 306\n",
    "    },\n",
    "    'EURNZD3': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 278\n",
    "    },\n",
    "    'GBPNZD': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 50,\n",
    "        'net pips profit': 224\n",
    "    },\n",
    "    'NZDCHF2': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 50,\n",
    "        'net pips profit': 140\n",
    "    },\n",
    "    'EURUSD3': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 56,\n",
    "        'net pips profit': 306\n",
    "    },\n",
    "    'GBPCAD': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.001,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 54,\n",
    "        'net pips profit': 151\n",
    "    },\n",
    "    'GBPUSD2': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.001,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 217\n",
    "    },\n",
    "    'EURGBP': {\n",
    "        'Lr': 0.001 ,\n",
    "        'Iterations': 188,\n",
    "        'depth': 7,\n",
    "        'accuracy' :52,\n",
    "        'pips net profit' : 97\n",
    "    },\n",
    "    'EURCAD2': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 51,\n",
    "        'pips net profit':  228\n",
    "    }, \n",
    "    'GBPCHF': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 53,\n",
    "        'pips net profit':  241\n",
    "    }, \n",
    "    'NZDUSD': {\n",
    "        'Iterations': 5, \n",
    "        'Lr': 0.77, \n",
    "        'depth': 7,\n",
    "        'accuracy': 54\n",
    "    }, \n",
    "    'EURUSD2': {\n",
    "        'Lr':0.025,\n",
    "        'Iterations': 450, \n",
    "        'depth': 5,\n",
    "        'accuracy': 53\n",
    "    },\n",
    "    'EURNZD2':{\n",
    "     'Iterations': 29, \n",
    "     'Lr': 0.63, \n",
    "     'depth': 7,\n",
    "     'accuracy': 50\n",
    "    }, \n",
    "    'GBPUSD': {\n",
    "     'Iterations': 13, \n",
    "     'Lr': 0.77, \n",
    "     'depth': 7,\n",
    "     'accuracy': 53\n",
    "    },\n",
    "    'GBPAUD': {\n",
    "    'Iterations': 13,\n",
    "    'Lr': 0.77,\n",
    "    'depth': 8,\n",
    "    'accuracy': 55       \n",
    "    },\n",
    "    'AUDUSD2': {\n",
    "    'Iterations': 5,\n",
    "    'Lr': 0.7,\n",
    "    'depth': 7,\n",
    "    'accuracy': 50       \n",
    "    },\n",
    "    'AUDCHF': {\n",
    "    'Iterations': 7,\n",
    "    'Lr': 0.71,\n",
    "    'depth' : 7\n",
    "    }, \n",
    "    'AUDCAD':{\n",
    "    'Iterations' : 9,\n",
    "    'Lr': 0.67,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'AUDNZD':{\n",
    "    'Iterations': 15,\n",
    "    'Lr': 0.68,\n",
    "    'depth': 7 \n",
    "    }, \n",
    "    'EURCAD':{ \n",
    "    'Iterations': 20,\n",
    "    'Lr': 0.039,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'NZDCHF' : {\n",
    "    'Iterations': 27,\n",
    "    'Lr' : 0.35,\n",
    "    'depth' : 7\n",
    "    },\n",
    "    'USDCAD': {\n",
    "    'Iterations': 20,\n",
    "    'Lr': 0.039,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'USDCHF':{\n",
    "    'Iterations': 188,\n",
    "    'Lr': 0.67,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'NZDCAD':{\n",
    "    'Iterations': 17,\n",
    "    'Lr': 0.38,\n",
    "    'depth': 7\n",
    "    }, \n",
    "    'AUDUSD' :{\n",
    "    'Iterations': 11 ,\n",
    "    'Lr': 0.211,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'EURUSD':{\n",
    "    'Iterations': 23,\n",
    "    'Lr': 0.51,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'EURNZD':{\n",
    "    'Iterations': 188,\n",
    "    'Lr' :0.55,\n",
    "    'depth': 7\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab691f3-3db3-4058-b981-81ca9fa363c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#\n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/primary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach.png?raw=true\" target=\"_blank\">\n",
    "    <img src=\"https://github.com/Aliyansayz/FxML_Research_v1/blob/main/images/primary/Multi%20Currency%20ML%20Powered%20Trading%20Using%20Regularized%20Hyperparameter%20Approach.png?raw=true\" width=\"600\" alt=\"Fx ML Research v1\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e8f12-3d27-4ebc-a977-2af709a2806a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Code To Load Train And Evaluate Forex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba94b2-df0e-480e-8618-2b33a4b784b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "def get_features_transformed(symbol, day_data, hour4_data, date_resume = '2020-06-01', X_test_re = False, temporal=True):\n",
    "    \n",
    "    \n",
    "    # date_resume\n",
    "    # split_date = pd.to_datetime('2020-06-01') \n",
    "    \n",
    "    split_date = pd.to_datetime(date_resume)\n",
    "\n",
    "    day_data = day_data.loc[split_date:]\n",
    "\n",
    "\n",
    "    # split_date = pd.to_datetime('2020-06-01')\n",
    "\n",
    "    hour4_data = hour4_data.loc[split_date:]\n",
    "\n",
    "\n",
    "    day_data['pips_change'] = np.subtract( day_data['Close'].to_numpy(), day_data['Close'].shift(1).to_numpy() )\n",
    "    \n",
    "    if 'JPY' in symbol: pips_change = day_data['pips_change'] * 10 ** 2 \n",
    "    \n",
    "    \n",
    "    else :  pips_change = day_data['pips_change'] * 10 ** 4 \n",
    "    \n",
    "    day_data['Target'] = pips_change\n",
    "    \n",
    "    window_size = 9\n",
    "\n",
    "    features = [f'Day_of_Week_T-{i}' for i in range(1, window_size + 1)] + [f'Week_of_Month_T-{i}' for i in range(1, window_size + 1)] +\\\n",
    "               [f'Month_T-{i}' for i in range(1, window_size + 1)]  +\\\n",
    "               [f'High_T-{i}' for i in range(1, window_size + 1) ] +  [f'Low_T-{i}'  for i in range(1, window_size + 1) ] + \\\n",
    "               [f'STDEV_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'RSI_T-{i}' for i in range(1, window_size + 1)   ] + \\\n",
    "               [f'Upper_Band_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'Lower_Band_T-{i}' for i in range(1, window_size + 1) ]+ \\\n",
    "               [f'candle_type_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'heikin_ashi_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'Price_Range_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'Median_Price_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'ema_3_T-{i}' for i in range(1, window_size + 1) ]  +\\\n",
    "               [f'ema_5_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'ema_7_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'ema_14_T-{i}'for i in range(1, window_size + 1) ] +\\\n",
    "               [f'ema_difference_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'supertrend_status_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'supertrend_crossover_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'supertrend_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'elastic_supertrend_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'elastic_supertrend_cross_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'elastic_supertrend_status_T-{i}' for i in range(1, window_size + 1) ]\n",
    "\n",
    "    if temporal == False : \n",
    "        # [f'Day_of_Week_T-{i}' for i in range(1, window_size + 1)]  +\n",
    "        features =  [f'High_T-{i}' for i in range(1, window_size + 1) ] +  [f'Low_T-{i}'  for i in range(1, window_size + 1) ] + \\\n",
    "               [f'STDEV_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'RSI_T-{i}' for i in range(1, window_size + 1)   ] +  [f'Upper_Band_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'Lower_Band_T-{i}' for i in range(1, window_size + 1) ]+ \\\n",
    "               [f'candle_type_T-{i}' for i in range(1, window_size + 1) ] + [f'heikin_ashi_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'Price_Range_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'Median_Price_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'ema_3_T-{i}' for i in range(1, window_size + 1) ] + [f'ema_5_T-{i}' for i in range(1, window_size + 1) ] + [f'ema_7_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'ema_14_T-{i}'for i in range(1, window_size + 1) ] + [f'ema_difference_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'supertrend_status_T-{i}' for i in range(1, window_size + 1) ] +\\\n",
    "               [f'supertrend_crossover_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'supertrend_T-{i}' for i in range(1, window_size + 1) ] + [f'elastic_supertrend_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'elastic_supertrend_cross_T-{i}' for i in range(1, window_size + 1) ] + \\\n",
    "               [f'elastic_supertrend_status_T-{i}' for i in range(1, window_size + 1) ]\n",
    "    day_features_X   =  day_data[features]\n",
    "\n",
    "    day_features_X = day_features_X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    day_features_X = day_features_X.astype(float)\n",
    "\n",
    "    hour4_features_X = hour4_data \n",
    "\n",
    "    hour4_features_X = hour4_features_X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    hour4_features_X = hour4_features_X.astype(float)\n",
    "    \n",
    "    # substrings = ['relative_range', 'candle_type', 'heikin_ashi', 'stdev', 'true' ] #, 'heikin_ashi',  'supertrend_h4']\n",
    "\n",
    "    substrings = ['relative_range', 'candle_type', 'heikin_ashi' ] #, 'heikin_ashi',  'supertrend_h4']\n",
    "\n",
    "    \n",
    "    # # Filter columns that contain any of the specified substrings\n",
    "    filtered_columns = [col for col in hour4_features_X.columns if any(sub in col for sub in substrings)]\n",
    "    hour4_features_X = hour4_features_X[filtered_columns]\n",
    "\n",
    "    # hour4_features_X = hour4_features_X.filter(like='relative_range')\n",
    "\n",
    "    X = day_features_X.join(hour4_features_X)\n",
    "    # X = day_features_X\n",
    "    \n",
    "    # if temporal == False :  X = day_features_X\n",
    "    \n",
    "    # X = day_features_X.join(hour4_features_X)\n",
    "\n",
    "    y = day_data['Target']\n",
    "\n",
    "    X.fillna(0.0, inplace=True)\n",
    "\n",
    "    y.fillna(0.0, inplace=True)\n",
    "\n",
    "    # print(len(data[features]))\n",
    "    X = X.astype(float)\n",
    "\n",
    "    div = int(len(X) * 0.8)\n",
    "    # train = :div\n",
    "    # test  = div:\n",
    "    X_train = X[:div]\n",
    "    X_test  = X[div:]\n",
    "\n",
    "    y_train = y[:div]\n",
    "    y_test  = y[div:]\n",
    "\n",
    "    # # Apply RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    # X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    \n",
    "    if not X_test_re:\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    else :\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, X_test\n",
    "\n",
    "def train_model_2(X_train_scaled, X_test_scaled, y_train, y_test ):\n",
    "    \n",
    "    # # Convert to Pool (CatBoost-specific data structure)\n",
    "    train_pool = Pool(X_train_scaled, y_train)\n",
    "    test_pool = Pool(X_test_scaled, y_test)\n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    # Initialize CatBoost regressor\n",
    "    model = CatBoostRegressor(iterations=300, early_stopping_rounds=75, \n",
    "                              learning_rate=0.1 , depth=5, verbose=100)\n",
    "\n",
    "    # Train the model using train pool\n",
    "    model.fit(train_pool)\n",
    "\n",
    "\n",
    "    # Make predictions using test pool\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def finetune_model(X_train_scaled, X_test_scaled, y_train, y_test, parameters = None ):\n",
    "\n",
    "#     model = CatBoostRegressor()\n",
    "#     parameters = {'depth' : [6,8,10],'learning_rate' : [0.01, 0.05, 0.1],\n",
    "#                   'iterations'    : [30, 50, 100]}\n",
    "\n",
    "#     model = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1)\n",
    "#     model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     # Make predictions using test pool\n",
    "#     y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "#     print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "        # Create the model\n",
    "    rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Define the parameter distribution\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create the RandomizedSearchCV object\n",
    "    model = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_dist,\n",
    "                                       n_iter=100, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error',\n",
    "                                       random_state=42)\n",
    "\n",
    "    # Fit the random search to the data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = random_search.best_params_\n",
    "    best_score  = random_search.best_score_\n",
    "\n",
    "    # Make predictions using test pool\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    \n",
    "    print(\"Best Parameters: \",best_params)\n",
    "    print(\"Best Score: \",best_score)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def train_model(X_train_scaled, X_test_scaled, y_train, y_test, parameters = None ):\n",
    "    \n",
    "    # train_model_1\n",
    "    # # Convert to Pool (CatBoost-specific data structure)\n",
    "    train_pool = Pool(X_train_scaled, y_train)\n",
    "    test_pool = Pool(X_test_scaled, y_test)\n",
    "    \n",
    "    \n",
    "    # np.random.seed(42)\n",
    "\n",
    "\n",
    "    # Initialize CatBoost regressor\n",
    "    if not parameters:\n",
    "        model = CatBoostRegressor(iterations=300, early_stopping_rounds=75, \n",
    "                              learning_rate=0.01 , depth=6, verbose=100)\n",
    "    \n",
    "    else : \n",
    "        iteration, learning_rate, depth = parameters[0], parameters[1], parameters[2]\n",
    "        model = CatBoostRegressor(iterations=iteration, early_stopping_rounds=75, \n",
    "                              learning_rate=learning_rate, depth=depth, verbose=100)\n",
    "        \n",
    "    # Train the model using train pool\n",
    "    model.fit(train_pool)\n",
    "\n",
    "\n",
    "    # Make predictions using test pool\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "def train_model_1(X_train_scaled, X_test_scaled, y_train, y_test ):\n",
    "    \n",
    "    # train_model_1\n",
    "    # # Convert to Pool (CatBoost-specific data structure)\n",
    "    train_pool = Pool(X_train_scaled, y_train)\n",
    "    test_pool = Pool(X_test_scaled, y_test)\n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    # Initialize CatBoost regressor\n",
    "    model = CatBoostRegressor(iterations=300, early_stopping_rounds=75, \n",
    "                              learning_rate=0.01 , depth=6, verbose=100)\n",
    "\n",
    "    # Train the model using train pool\n",
    "    model.fit(train_pool)\n",
    "\n",
    "\n",
    "    # Make predictions using test pool\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_features_files(hour4_lag_3=None):\n",
    "\n",
    "    day_features_path = 'day_features_data.bin'\n",
    "\n",
    "    with open(day_features_path, 'rb') as file :\n",
    "\n",
    "        day_features = pickle.load(file)\n",
    "\n",
    "\n",
    "    # hour4_features_path = 'hour4_features_data.bin'  \n",
    "    hour4_features_path = 'hour4_features_data_lag_by_7.bin'\n",
    "    \n",
    "    if hour4_lag_3 == True : hour4_features_path = 'hour4_features_data_lag_by_3.bin'\n",
    "     \n",
    "    with open(hour4_features_path, 'rb') as file :\n",
    "\n",
    "        hour4_features = pickle.load(file)\n",
    "        \n",
    "    return  day_features, hour4_features\n",
    "    \n",
    "\n",
    "def evaluate_model_old(symbol, model, X_test_scaled,  y_test):\n",
    "    \n",
    "    sample = 0\n",
    "    right  = 0\n",
    "    wrong  = 0\n",
    "    status = []\n",
    "    pips_profit = 0 \n",
    "    pips_loss = 0\n",
    "\n",
    "    for point in range(1, 22 + 1 ):\n",
    "\n",
    "        sample += 1\n",
    "        y_pred = model.predict(X_test_scaled[-point])\n",
    "        y_true = y_test[-point]\n",
    "\n",
    "        if y_pred < 0.0 and y_true < 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        elif y_pred > 0.0 and y_true > 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        else:\n",
    "            pips_loss += abs(y_true)\n",
    "            wrong += 1\n",
    "            status.append(0)\n",
    "\n",
    "    # y_pred = model.predict(test_pool )\n",
    "\n",
    "        # print(\"actual : \",y_test[-3],\"\\npredicted : \",y_pred )\n",
    "    print(f\"{symbol}\\n\")\n",
    "    print(\"accuracy on last 22 test points Month May 2024 with low learning rate 2020 dataset 80% training dataset is \",right/sample * 100 )\n",
    "\n",
    "    # print(\"accuracy on last 100 test points is \",right) #wrong/sample * 100 )\n",
    "    print(status)\n",
    "    print(f\"Pips On Profit Side was : \",pips_profit)\n",
    "    print(\"Pips On Loss Side was : \",pips_loss)\n",
    "\n",
    "    print(\"net pips profit : \", pips_profit-pips_loss )\n",
    "    \n",
    "    \n",
    "    status = []\n",
    "    for point in range(1, len(X_test_scaled) + 1 ):\n",
    "\n",
    "        sample += 1\n",
    "        y_pred = model.predict(X_test_scaled[-point])\n",
    "        y_true = y_test[-point]\n",
    "\n",
    "        if y_pred < 0.0 and y_true < 0.0 :\n",
    "            # pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        elif y_pred > 0.0 and y_true > 0.0 :\n",
    "            # pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        else:\n",
    "            # pips_loss += abs(y_true)\n",
    "            wrong += 1\n",
    "            status.append(0)\n",
    "\n",
    "    print(\"accuracy on all test points with low learning rate 2020 dataset 80% training dataset is \",right/sample * 100 )\n",
    "\n",
    "    # print(\"accuracy on last 100 test points is \",right) #wrong/sample * 100 )\n",
    "    print(status)\n",
    "    \n",
    "    \n",
    "    print(f\"{symbol}\\n==============================\")\n",
    "    \n",
    "    \n",
    "def save_model(symbol, model): \n",
    "    \n",
    "    model.save_model(f\"forex_models/{symbol}_model_pips_change\")\n",
    "\n",
    "    \n",
    "    \n",
    "# def load_features_files():\n",
    "\n",
    "#     day_features_path = 'day_features_data.bin'\n",
    "\n",
    "#     with open(day_features_path, 'rb') as file :\n",
    "\n",
    "#         day_features = pickle.load(file)\n",
    "\n",
    "\n",
    "#     # hour4_features_path = 'hour4_features_data.bin'  \n",
    "#     hour4_features_path = 'hour4_features_data_lag_by_7.bin'\n",
    "\n",
    "#     with open(hour4_features_path, 'rb') as file :\n",
    "\n",
    "#         hour4_features = pickle.load(file)\n",
    "        \n",
    "#     return  day_features, hour4_features\n",
    "    \n",
    "    \n",
    "def get_day_hour4_features(symbol, day_features, hour4_features):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for day_pair in day_features: \n",
    "        if  day_pair['symbol'] == symbol: \n",
    "\n",
    "            day_data = day_pair['day_features'] \n",
    "            break\n",
    "\n",
    "\n",
    "    for hour4_pair in hour4_features: \n",
    "        if  hour4_pair['symbol'] == symbol: \n",
    "\n",
    "            hour4_data = hour4_pair['hour4_features'] \n",
    "            break\n",
    "    \n",
    "    return  day_data, hour4_data\n",
    "\n",
    "def load_model(symbol, path = None):\n",
    "    \n",
    "    \n",
    "    if path: \n",
    "        pass\n",
    "        path = f\"{path}/{symbol}_model_pips_change\"\n",
    "        \n",
    "    else: \n",
    "        path = f\"forex_models/{symbol}_model_pips_change\"\n",
    "    \n",
    "    model = CatBoostRegressor()\n",
    "    \n",
    "    model.load_model(path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_old(symbol):\n",
    "    \n",
    "    pass\n",
    "    model = CatBoostRegressor()\n",
    "    path = f'forex_models_low_lr_few_h4features_300 iteration/{symbol}_model_pips_change'\n",
    "    model.load_model(path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(symbol, model, X_test_scaled, y_test):\n",
    "    \n",
    "    sample = 0\n",
    "    right  = 0\n",
    "    wrong  = 0\n",
    "    status = []\n",
    "    pips_profit = 0 \n",
    "    pips_loss = 0\n",
    "\n",
    "    for point in range(23, 45 + 1 ): # may 1 - 22 # april 23 - 55\n",
    "\n",
    "        sample += 1\n",
    "        y_pred = model.predict(X_test_scaled[-point])\n",
    "        y_true = y_test[-point]\n",
    "\n",
    "        if y_pred < 0.0 and y_true < 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        elif y_pred > 0.0 and y_true > 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        else:\n",
    "            pips_loss += abs(y_true)\n",
    "            wrong += 1\n",
    "            status.append(0)\n",
    "\n",
    "    # y_pred = model.predict(test_pool )\n",
    "\n",
    "        # print(\"actual : \",y_test[-3],\"\\npredicted : \",y_pred )\n",
    "    print(f\"{symbol}\\n\")\n",
    "    print(\"accuracy on last 22 test points Month May 2024 with low learning rate 2020 dataset 80% training dataset is \",right/sample * 100 )\n",
    "\n",
    "    # print(\"accuracy on last 100 test points is \",right) #wrong/sample * 100 )\n",
    "    print(status)\n",
    "    print(f\"Pips On Profit Side was : \",pips_profit)\n",
    "    print(\"Pips On Loss Side was : \",pips_loss)\n",
    "\n",
    "    print(\"net pips profit : \", pips_profit-pips_loss )\n",
    "    \n",
    "    status = []\n",
    "    for point in range(1, len(X_test_scaled) + 1 ):\n",
    "\n",
    "        sample += 1\n",
    "        y_pred = model.predict(X_test_scaled[-point])\n",
    "        y_true = y_test[-point]\n",
    "\n",
    "        if y_pred < 0.0 and y_true < 0.0 :\n",
    "            # pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        elif y_pred > 0.0 and y_true > 0.0 :\n",
    "            # pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        else:\n",
    "            # pips_loss += abs(y_true)\n",
    "            wrong += 1\n",
    "            status.append(0)\n",
    "\n",
    "    print(\"accuracy on all test points excluding last 22 points with low learning rate 2020 dataset 80% training dataset is \",right/sample * 100 )\n",
    "\n",
    "    # print(\"accuracy on last 100 test points is \",right) #wrong/sample * 100 )\n",
    "    print(status)\n",
    "    \n",
    "    \n",
    "    print(f\"{symbol}\\n==============================\")\n",
    "    # try:\n",
    "    #     return  pips_profit-pips_loss, right/sample * 100\n",
    "    # except : pass\n",
    "\n",
    "    # net_gains , accuracy\n",
    "    \n",
    "    \n",
    "def evaluate_model_return_gains_accuracy(symbol, model, X_test_scaled, y_test, custom_sample= None, X_test = None ):\n",
    "    \n",
    "    sample = 0\n",
    "    right  = 0\n",
    "    wrong  = 0\n",
    "    status = []\n",
    "    pips_profit = 0\n",
    "    pips_loss = 0\n",
    "    \n",
    "    # custom_sample=[1,22]\n",
    "    if custom_sample:\n",
    "        step, cluster = custom_sample[0] , custom_sample[1] \n",
    "        start , end = 0 , 0\n",
    "        # for i in range(step):\n",
    "        #     start += 1 + end # 23\n",
    "        #     end   += cluster      # \n",
    "        \n",
    "        for i in range(step): # 1-> 22 + 1 # 2\n",
    "            end   += cluster\n",
    "        end += 1\n",
    "        start = end - cluster\n",
    "        \n",
    "    else : start, end = 1, 22\n",
    "    \n",
    "#     # Convert net gains to numpy array\n",
    "# returns = np.array(net_gains)\n",
    "\n",
    "# # Calculate mean return\n",
    "# mean_return = np.mean(returns)\n",
    "\n",
    "# # Calculate standard deviation of returns\n",
    "# std_return = np.std(returns)\n",
    "\n",
    "# # Assuming risk-free rate is 0 for simplicity\n",
    "# risk_free_rate = 0\n",
    "\n",
    "# # Calculate Sharpe ratio\n",
    "# sharpe_ratio = (mean_return - risk_free_rate) / std_return\n",
    "\n",
    "# print(f'Sharpe Ratio: {sharpe_ratio:.2f}')\n",
    "\n",
    "\n",
    "    net_gains_list = []\n",
    "    for point in range(start, end  ): # may 1 - 22 # april 23 - 45\n",
    "\n",
    "        sample += 1\n",
    "        y_pred = model.predict(X_test_scaled[-point])\n",
    "        y_true = y_test[-point]\n",
    "\n",
    "        if y_pred < 0.0 and y_true < 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            net_gains_list.append(abs(y_true))\n",
    "            status.append(-1)\n",
    "\n",
    "        elif y_pred > 0.0 and y_true > 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            net_gains_list.append(y_true)\n",
    "            status.append(1)\n",
    "\n",
    "        else:\n",
    "            pips_loss += abs(y_true)\n",
    "            wrong += 1\n",
    "            # status.append(0)\n",
    "            net_gains_list.append(-1*abs(y_true))\n",
    "\n",
    "            if y_pred > 0.0 and y_true < 0.0 : status.append(str('+1-1'))\n",
    "\n",
    "            elif y_pred < 0.0 and y_true > 0.0 : status.append(str('-1+1'))\n",
    "        \n",
    "\n",
    "    # y_pred = model.predict(test_pool )\n",
    "    \n",
    "    # print(\"actual : \",y_test[-3],\"\\npredicted : \",y_pred )\n",
    "    # print(f\"{symbol}\\n\")\n",
    "    # print(\"accuracy on last 22 test points Month May 2024 with low learning rate 2020 dataset 80% training dataset is \",right/sample * 100 )\n",
    "    \n",
    "    returns = np.array(net_gains_list)\n",
    "\n",
    "    # Calculate mean return\n",
    "    mean_return = np.mean(returns)\n",
    "\n",
    "    # Calculate standard deviation of returns\n",
    "    std_return = np.std(returns)\n",
    "\n",
    "    # Assuming risk-free rate is 0 for simplicity\n",
    "    risk_free_rate = 0\n",
    "\n",
    "    # Calculate Sharpe ratio\n",
    "    sharpe_ratio = (mean_return - risk_free_rate) / std_return\n",
    "    if len(X_test) != None :\n",
    "        date   = X_test.index.values[-end]\n",
    "        date = str(date.astype('datetime64[D]'))\n",
    "        print(symbol , f\" Starting date : {date} \")\n",
    "        \n",
    "    status = status[::-1] # starting from end because end is the first most day of the chosen month\n",
    "    \n",
    "    print(status)\n",
    "    # print(\"accuracy on last 100 test points is \",right) #wrong/sample * 100 )\n",
    "    accuracy_by_days = right/sample * 100\n",
    "    # print(f\"Accuracy by days :\",accuracy_by_days)\n",
    "    print(f'Sharpe Ratio: {sharpe_ratio:.2f}')\n",
    "    \n",
    "    print(f\"Pips On Profit Side was : \",pips_profit)\n",
    "    print(\"Pips On Loss Side was : \",pips_loss)\n",
    "    accuracy_by_net_gains = pips_profit / (pips_profit+pips_loss)*100\n",
    "    print('accuracy by days', accuracy_by_days )\n",
    "    \n",
    "    print('accuracy_by_net_gains ',accuracy_by_net_gains )\n",
    "    net_gains = pips_profit-pips_loss\n",
    "#     print(\"net pips profit : \", pips_profit-pips_loss )\n",
    "    \n",
    "    status = []\n",
    "    right, wrong = 0, 0\n",
    "    sample = 0\n",
    "    pips_profit, pips_loss = 0, 0\n",
    "    for point in range(end, len(X_test_scaled) + 1 ): # excluding May April 55  # excluding May 23 -> till end of length\n",
    "\n",
    "        sample += 1\n",
    "        y_pred = model.predict(X_test_scaled[-point])\n",
    "        y_true = y_test[-point]\n",
    "\n",
    "        if y_pred < 0.0 and y_true < 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(-1)\n",
    "\n",
    "        elif y_pred > 0.0 and y_true > 0.0 :\n",
    "            pips_profit += abs(y_true)\n",
    "            right += 1\n",
    "            status.append(1)\n",
    "\n",
    "        else:\n",
    "            pips_loss += abs(y_true)\n",
    "            wrong += 1\n",
    "            # status.append(0)\n",
    "            if y_pred > 0.0 and y_true < 0.0 : status.append(str('+1-1'))\n",
    "\n",
    "            elif y_pred < 0.0 and y_true > 0.0 : status.append(str('-1+1'))\n",
    "\n",
    "    # print(\"accuracy on all test points excluding last 22 points with low learning rate 2020 dataset 80% training dataset is \",right/sample * 100 )\n",
    "\n",
    "    # print(\"accuracy on last 100 test points is \",right) #wrong/sample * 100 )\n",
    "    # print(status)\n",
    "    accuracy_by_days_before = right/sample * 100\n",
    "    accuracy_by_net_gains_before = pips_profit / (pips_profit+pips_loss)*100\n",
    "    gains = {}\n",
    "    gains['net_gains'] = net_gains\n",
    "    gains['accuracy_by_days']  = accuracy_by_days\n",
    "    gains['accuracy_by_days_before']  = accuracy_by_days_before\n",
    "    gains['accuracy_by_net_gains_before'] = accuracy_by_net_gains_before\n",
    "    gains['accuracy_by_net_gains'] = accuracy_by_net_gains\n",
    "    gains['sharpe_ratio'] = sharpe_ratio\n",
    "    \n",
    "    # print(f\"{symbol}\\n==============================\")\n",
    "    try:\n",
    "        return  gains, accuracy_by_days_before\n",
    "    except : pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_profit_and_loss(pl_info, date, pips_profit, pips_loss, net_gain):\n",
    "    if date in pl_info:\n",
    "        pl_info[date]['pips_profit'] += pips_profit\n",
    "        pl_info[date]['pips_loss'] += pips_loss\n",
    "        pl_info[date]['net_gain'] += net_gain\n",
    "    else:\n",
    "        pl_info[date] = {'pips_profit': pips_profit, 'pips_loss': pips_loss, 'net_gain': net_gain}\n",
    "\n",
    "    return pl_info\n",
    "        \n",
    "\n",
    "def each_day_gain_loss(day_features, hour4_features, custom_sample= None , model_path = None):\n",
    "\n",
    "    pass\n",
    "    # custom_sample = [ 1, 22 ]  \n",
    "    forex_pairs = [\n",
    "    'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY']\n",
    "    symbols_store = {}\n",
    "    \n",
    "    sample = 0\n",
    "    right  = 0\n",
    "    wrong  = 0\n",
    "    status = []\n",
    "    pips_profit = 0\n",
    "    pips_loss = 0\n",
    "    pl_info = {}\n",
    "    # custom_sample=[1,22]\n",
    "    if custom_sample :\n",
    "        step, cluster = custom_sample[0] , custom_sample[1] \n",
    "        start , end = 0 , 0\n",
    "        for i in range(step):\n",
    "            end   += cluster + 1\n",
    "\n",
    "        start = end - cluster\n",
    "    \n",
    "    \n",
    "    else : start, end = 1, 22\n",
    "    \n",
    "    \n",
    "    for point in range(start, end + 1 ): # may 1 - 22 # april 23 - 45\n",
    "        \n",
    "        pips_profit, pips_loss = 0, 0\n",
    "        net_gains = 0\n",
    "        \n",
    "        sample += 1\n",
    "        for symbol in forex_pairs:\n",
    "    \n",
    "            \n",
    "            if symbol not in symbols_store:\n",
    "            \n",
    "                day_data, hour4_data = get_day_hour4_features(symbol, day_features, hour4_features)\n",
    "                X_train_scaled, X_test_scaled, y_train, y_test, X_test = get_features_transformed(symbol, day_data, hour4_data, date_resume='2020-06-01',  X_test_re=True)\n",
    "                \n",
    "                if model_path: model = load_model(symbol, path = model_path)\n",
    "                else : model = load_model(symbol)\n",
    "                \n",
    "                \n",
    "                symbols_store[symbol] = { 'X_test_scaled': X_test_scaled,  \n",
    "                                         'y_test': y_test, 'model': model, 'X_test': X_test  }\n",
    "                # symbols_store[symbol] = { 'y_test': y_test }\n",
    "                # symbols_store[symbol] = { 'model': model }\n",
    "                # symbols_store[symbol] = { 'X_test': X_test }\n",
    "            \n",
    "            else:\n",
    "                X_test_scaled = symbols_store[symbol]['X_test_scaled']\n",
    "                y_test = symbols_store[symbol]['y_test']\n",
    "                model  = symbols_store[symbol]['model']\n",
    "                X_test = symbols_store[symbol]['X_test']\n",
    "            \n",
    "            y_pred = model.predict(X_test_scaled[-point])\n",
    "            y_true = y_test[-point]\n",
    "            date   = X_test.index.values[-point]\n",
    "            # date = np.datetime64(date)\n",
    "            date = str(date.astype('datetime64[D]'))\n",
    "            \n",
    "            if y_pred < 0.0 and y_true < 0.0 :\n",
    "                pips_profit += abs(y_true)\n",
    "                right += 1\n",
    "                status.append(-1)\n",
    "\n",
    "            elif y_pred > 0.0 and y_true > 0.0 :\n",
    "                pips_profit += abs(y_true)\n",
    "                right += 1\n",
    "                status.append(1)\n",
    "\n",
    "            else:\n",
    "                pips_loss += abs(y_true)\n",
    "                wrong += 1\n",
    "                # status.append(0)\n",
    "                if y_pred > 0.0 and y_true < 0.0 : status.append(str('+1-1'))\n",
    "\n",
    "                elif y_pred < 0.0 and y_true > 0.0 : status.append(str('-1+1'))\n",
    "            \n",
    "        net_gain = int(pips_profit - pips_loss)\n",
    "        pl_info  =  update_profit_and_loss(pl_info, date, int(pips_profit), int(pips_loss), net_gain)\n",
    "\n",
    "\n",
    "    return pl_info\n",
    "\n",
    "\n",
    "def update_symbol_profit_and_loss(pl_info, date, pips_profit, pips_loss, net_gain, symbol):\n",
    "      \n",
    "#     symbol\n",
    "    # if 'date_info' not in pl_info:  pl_info['date_info'] = date\n",
    "#     if symbol in pl_info:\n",
    "#         pl_info[symbol]['pips_profit'] += pips_profit\n",
    "#         pl_info[symbol]['pips_loss'] += pips_loss\n",
    "#         pl_info[symbol]['net_gain'] += net_gain\n",
    "        \n",
    "#     else:\n",
    "#         # pl_info[date] = {'pips_profit': pips_profit, 'pips_loss': pips_loss, 'net_gain': net_gain}\n",
    "    pl_info[symbol] = {'pips_profit': pips_profit, 'pips_loss': pips_loss, 'net_gain': net_gain, 'date': date}\n",
    "\n",
    "        \n",
    "    return pl_info\n",
    "\n",
    "# '2024-01-29': {'pips_profit': 137, 'pips_loss': 1127, 'net_gain': -989} \n",
    "\n",
    "\n",
    "\n",
    "def each_symbol_gain_loss_by_date(day_features, hour4_features, custom_sample= None , model_path = None, date_val='2024-01-29'):\n",
    "\n",
    "    pass\n",
    "    # custom_sample = [ 1, 22 ]  \n",
    "    forex_pairs = [\n",
    "    'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY']\n",
    "    symbols_store = {}\n",
    "    \n",
    "    sample = 0\n",
    "    right  = 0\n",
    "    wrong  = 0\n",
    "    status = []\n",
    "    pips_profit = 0\n",
    "    pips_loss = 0\n",
    "    pl_info = {}\n",
    "    # custom_sample=[1,22]\n",
    "    if custom_sample :\n",
    "        step, cluster = custom_sample[0] , custom_sample[1] \n",
    "        start , end = 0 , 0\n",
    "        for i in range(step):\n",
    "            end   += cluster + 1\n",
    "\n",
    "        start = end - cluster\n",
    "    \n",
    "    \n",
    "    else : start, end = 1, 22\n",
    "    \n",
    "    \n",
    "    for point in range(start, end + 1 ): # may 1 - 22 # april 23 - 45\n",
    "        \n",
    "        pips_profit, pips_loss = 0, 0\n",
    "        net_gains = 0\n",
    "        \n",
    "        sample += 1\n",
    "        for symbol in forex_pairs:\n",
    "            \n",
    "            pips_profit, pips_loss = 0, 0\n",
    "            if symbol not in symbols_store:\n",
    "            \n",
    "                day_data, hour4_data = get_day_hour4_features(symbol, day_features, hour4_features)\n",
    "                X_train_scaled, X_test_scaled, y_train, y_test, X_test = get_features_transformed(symbol, day_data, hour4_data, date_resume='2020-06-01',  X_test_re=True)\n",
    "                \n",
    "                if model_path: model = load_model(symbol, path = model_path)\n",
    "                else : model = load_model(symbol)\n",
    "                \n",
    "                \n",
    "                symbols_store[symbol] = { 'X_test_scaled': X_test_scaled,  \n",
    "                                         'y_test': y_test, 'model': model, 'X_test': X_test  }\n",
    "                # symbols_store[symbol] = { 'y_test': y_test }\n",
    "                # symbols_store[symbol] = { 'model': model }\n",
    "                # symbols_store[symbol] = { 'X_test': X_test }\n",
    "            \n",
    "            else:\n",
    "                X_test_scaled = symbols_store[symbol]['X_test_scaled']\n",
    "                y_test = symbols_store[symbol]['y_test']\n",
    "                model  = symbols_store[symbol]['model']\n",
    "                X_test = symbols_store[symbol]['X_test']\n",
    "            \n",
    "            \n",
    "            date   = X_test.index.values[-point]\n",
    "            # date = np.datetime64(date)\n",
    "            date = str(date.astype('datetime64[D]'))\n",
    "            \n",
    "            if date != date_val : break\n",
    "            \n",
    "            y_pred = model.predict(X_test_scaled[-point])\n",
    "            y_true = y_test[-point]\n",
    "            \n",
    "            if y_pred < 0.0 and y_true < 0.0 :\n",
    "                pips_profit = abs(y_true)\n",
    "                right += 1\n",
    "                status.append(-1)\n",
    "\n",
    "            elif y_pred > 0.0 and y_true > 0.0 :\n",
    "                pips_profit = abs(y_true)\n",
    "                right += 1\n",
    "                status.append(1)\n",
    "\n",
    "            else:\n",
    "                pips_loss = abs(y_true)\n",
    "                wrong += 1\n",
    "                # status.append(0)\n",
    "                if y_pred > 0.0 and y_true < 0.0 : status.append(str('+1-1'))\n",
    "\n",
    "                elif y_pred < 0.0 and y_true > 0.0 : status.append(str('-1+1'))\n",
    "            \n",
    "            net_gain = int(pips_profit - pips_loss) # net gain on symbol basis for a particular day\n",
    "            pl_info  =  update_symbol_profit_and_loss(pl_info, date, int(pips_profit), int(pips_loss), net_gain, symbol)\n",
    "\n",
    "\n",
    "    return pl_info\n",
    "\n",
    "\n",
    "\n",
    "def each_day_gain_loss_with_threshold(day_features, hour4_features, custom_sample= None , model_path = None, threshold = 5 ):\n",
    "\n",
    "    pass\n",
    "    # custom_sample = [ 1, 22 ]  \n",
    "    forex_pairs = [\n",
    "    'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY']\n",
    "    symbols_store = {}\n",
    "    \n",
    "    sample = 0\n",
    "    right  = 0\n",
    "    wrong  = 0\n",
    "    status = []\n",
    "    pips_profit = 0\n",
    "    pips_loss = 0\n",
    "    pl_info = {}\n",
    "    # custom_sample=[1,22]\n",
    "    if custom_sample :\n",
    "        step, cluster = custom_sample[0] , custom_sample[1] \n",
    "        start , end = 0 , 0\n",
    "        for i in range(step):\n",
    "            end   += cluster + 1\n",
    "\n",
    "        start = end - cluster\n",
    "    \n",
    "    \n",
    "    else : start, end = 1, 22\n",
    "    \n",
    "    era = start + 10\n",
    "    \n",
    "    for point in range(start, end + 1 ): # may 1 - 22 # april 23 - 45\n",
    "        \n",
    "        pips_profit, pips_loss = 0, 0\n",
    "        net_gains = 0\n",
    "        \n",
    "        sample += 1\n",
    "        for symbol in forex_pairs:\n",
    "    \n",
    "            \n",
    "            if symbol not in symbols_store:\n",
    "            \n",
    "                day_data, hour4_data = get_day_hour4_features(symbol, day_features, hour4_features)\n",
    "                X_train_scaled, X_test_scaled, y_train, y_test, X_test = get_features_transformed(symbol, day_data, hour4_data, date_resume='2020-06-01',  X_test_re=True)\n",
    "                \n",
    "                if model_path: model = load_model(symbol, path = model_path)\n",
    "                else : model = load_model(symbol)\n",
    "                \n",
    "                \n",
    "                symbols_store[symbol] = { 'X_test_scaled': X_test_scaled,  \n",
    "                                         'y_test': y_test, 'model': model, 'X_test': X_test  }\n",
    "                # symbols_store[symbol] = { 'y_test': y_test }\n",
    "                # symbols_store[symbol] = { 'model': model }\n",
    "                # symbols_store[symbol] = { 'X_test': X_test }\n",
    "            \n",
    "            else:\n",
    "                X_test_scaled = symbols_store[symbol]['X_test_scaled']\n",
    "                y_test = symbols_store[symbol]['y_test']\n",
    "                model  = symbols_store[symbol]['model']\n",
    "                X_test = symbols_store[symbol]['X_test']\n",
    "            \n",
    "            y_pred = model.predict(X_test_scaled[-point])\n",
    "            y_true = y_test[-point]\n",
    "            date   = X_test.index.values[-point]\n",
    "            # date = np.datetime64(date)\n",
    "            date = str(date.astype('datetime64[D]'))\n",
    "            if point < era : \n",
    "                if abs(y_pred) < threshold : continue\n",
    "            \n",
    "            if y_pred < 0.0 and y_true < 0.0 :\n",
    "                pips_profit += abs(y_true)\n",
    "                right += 1\n",
    "                status.append(-1)\n",
    "\n",
    "            elif y_pred > 0.0 and y_true > 0.0 :\n",
    "                pips_profit += abs(y_true)\n",
    "                right += 1\n",
    "                status.append(1)\n",
    "\n",
    "            else:\n",
    "                pips_loss += abs(y_true)\n",
    "                wrong += 1\n",
    "                # status.append(0)\n",
    "                if y_pred > 0.0 and y_true < 0.0 : status.append(str('+1-1'))\n",
    "\n",
    "                elif y_pred < 0.0 and y_true > 0.0 : status.append(str('-1+1'))\n",
    "            \n",
    "        net_gain = int(pips_profit - pips_loss)\n",
    "        pl_info  =  update_profit_and_loss(pl_info, date, int(pips_profit), int(pips_loss), net_gain)\n",
    "\n",
    "\n",
    "    return pl_info\n",
    "\n",
    "#     accuracy_by_days = right/sample * 100\n",
    "\n",
    "#     accuracy_by_net_gains = pips_profit / (pips_profit+pips_loss)*100\n",
    "\n",
    "#     net_gains = pips_profit-pips_loss\n",
    "\n",
    "    \n",
    "import pickle\n",
    "\n",
    "parameters = {\n",
    "    'NZDJPY': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.001,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 6\n",
    "    },\n",
    "    'AUDJPY': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.01,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 6\n",
    "    },\n",
    "    'USDJPY': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.01,\n",
    "        'depth': 6\n",
    "    },\n",
    "    'CADJPY': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.01,\n",
    "        'depth': 6\n",
    "    },\n",
    "    'NZDCAD': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 114\n",
    "    },\n",
    "    'CADCHF': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 158\n",
    "    },\n",
    "    'USDCHF2': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 50,\n",
    "        'net pips profit': 179\n",
    "    },\n",
    "    'EURAUD': {\n",
    "        'Iterations': 7,\n",
    "        'Lr': 0.06,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 56,\n",
    "        'net pips profit': 306\n",
    "    },\n",
    "    'EURNZD3': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 278\n",
    "    },\n",
    "    'GBPNZD': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 50,\n",
    "        'net pips profit': 224\n",
    "    },\n",
    "    'NZDCHF2': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 50,\n",
    "        'net pips profit': 140\n",
    "    },\n",
    "    'EURUSD3': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 56,\n",
    "        'net pips profit': 306\n",
    "    },\n",
    "    'GBPCAD': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.001,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 54,\n",
    "        'net pips profit': 151\n",
    "    },\n",
    "    'GBPUSD2': {\n",
    "        'Iterations': 5,\n",
    "        'Lr': 0.001,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 52,\n",
    "        'net pips profit': 217\n",
    "    },\n",
    "    'EURGBP': {\n",
    "        'Lr': 0.001 ,\n",
    "        'Iterations': 188,\n",
    "        'depth': 7,\n",
    "        'accuracy' :52,\n",
    "        'pips net profit' : 97\n",
    "    },\n",
    "    'EURCAD2': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 7,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 51,\n",
    "        'pips net profit':  228\n",
    "    }, \n",
    "    'GBPCHF': {\n",
    "        'Iterations': 15,\n",
    "        'Lr': 0.1,\n",
    "        'scaler': 'RobustScaler()',\n",
    "        'depth': 8,\n",
    "        'window_size': 9,\n",
    "        'accuracy': 53,\n",
    "        'pips net profit':  241\n",
    "    }, \n",
    "    'NZDUSD': {\n",
    "        'Iterations': 5, \n",
    "        'Lr': 0.77, \n",
    "        'depth': 7,\n",
    "        'accuracy': 54\n",
    "    }, \n",
    "    'EURUSD2': {\n",
    "        'Lr':0.025,\n",
    "        'Iterations': 450, \n",
    "        'depth': 5,\n",
    "        'accuracy': 53\n",
    "    },\n",
    "    'EURNZD2':{\n",
    "     'Iterations': 29, \n",
    "     'Lr': 0.63, \n",
    "     'depth': 7,\n",
    "     'accuracy': 50\n",
    "    }, \n",
    "    'GBPUSD': {\n",
    "     'Iterations': 13, \n",
    "     'Lr': 0.77, \n",
    "     'depth': 7,\n",
    "     'accuracy': 53\n",
    "    },\n",
    "    'GBPAUD': {\n",
    "    'Iterations': 13,\n",
    "    'Lr': 0.77,\n",
    "    'depth': 8,\n",
    "    'accuracy': 55       \n",
    "    },\n",
    "    'AUDUSD2': {\n",
    "    'Iterations': 5,\n",
    "    'Lr': 0.7,\n",
    "    'depth': 7,\n",
    "    'accuracy': 50       \n",
    "    },\n",
    "    'AUDCHF': {\n",
    "    'Iterations': 7,\n",
    "    'Lr': 0.71,\n",
    "    'depth' : 7\n",
    "    }, \n",
    "    'AUDCAD':{\n",
    "    'Iterations' : 9,\n",
    "    'Lr': 0.67,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'AUDNZD':{\n",
    "    'Iterations': 15,\n",
    "    'Lr': 0.68,\n",
    "    'depth': 7 \n",
    "    }, \n",
    "    'EURCAD':{ \n",
    "    'Iterations': 20,\n",
    "    'Lr': 0.039,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'NZDCHF' : {\n",
    "    'Iterations': 27,\n",
    "    'Lr' : 0.35,\n",
    "    'depth' : 7\n",
    "    },\n",
    "    'USDCAD': {\n",
    "    'Iterations': 20,\n",
    "    'Lr': 0.039,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'USDCHF':{\n",
    "    'Iterations': 188,\n",
    "    'Lr': 0.67,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'NZDCAD':{\n",
    "    'Iterations': 17,\n",
    "    'Lr': 0.38,\n",
    "    'depth': 7\n",
    "    }, \n",
    "    'AUDUSD' :{\n",
    "    'Iterations': 11 ,\n",
    "    'Lr': 0.211,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'EURUSD':{\n",
    "    'Iterations': 23,\n",
    "    'Lr': 0.51,\n",
    "    'depth': 7\n",
    "    },\n",
    "    'EURNZD':{\n",
    "    'Iterations': 188,\n",
    "    'Lr' :0.55,\n",
    "    'depth': 7\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('parameters', 'wb') as file:\n",
    "    pickle.dump(parameters, file )\n",
    "    \n",
    "\n",
    "def load_parameters(symbol):\n",
    "    \n",
    "    with open('parameters', 'rb') as file:\n",
    "        parameters = pickle.load(file )\n",
    "        \n",
    "    return parameters[symbol]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f2662-b410-460a-a85b-85ea8d7757dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Python Code To Make A Binary File From All Daily OHLC Currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8b88b-9629-40ae-82a7-eef672a138c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "forex_pairs = [\n",
    "    'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY'\n",
    "        ]\n",
    "\n",
    "\n",
    "currency_ohlc = []\n",
    "count = 0\n",
    "for pair in forex_pairs:\n",
    "    daily_path = f'currency_data/{pair}_Daily.csv'\n",
    "    data = return_df_day(daily_path)\n",
    "    data = add_features(data)\n",
    "    data = make_features_lagged( lag_by= 9)\n",
    "\n",
    "    currency_df = { \"day_features\": data, \"symbol\": f\"{pair}\"  }\n",
    "    currency_ohlc.append(currency_df)\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('day_features_data.bin', 'wb') as file:\n",
    "    pickle.dump(currency_ohlc, file)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894789d1-5631-401d-acc6-3c13ed282822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate RSI\n",
    "def calculate_rsi(series, period=5):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "def ema( price, period):\n",
    "\n",
    "  price = np.array(price)\n",
    "  alpha = 2 / (period + 1.0)\n",
    "  alpha_reverse = 1 - alpha\n",
    "  data_length = len(price)\n",
    "\n",
    "  power_factors = alpha_reverse ** (np.arange(data_length + 1))\n",
    "  initial_offset = price[0] * power_factors[1:]\n",
    "\n",
    "  scale_factors = 1 / power_factors[:-1]\n",
    "\n",
    "  weight_factor = alpha * alpha_reverse ** (data_length - 1)\n",
    "\n",
    "  weighted_price_data = price * weight_factor * scale_factors\n",
    "  cumulative_sums = weighted_price_data.cumsum()\n",
    "  ema_values = initial_offset + cumulative_sums * scale_factors[::-1]\n",
    "\n",
    "  return ema_values\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def candle_type(o, h, l, c):\n",
    "\n",
    "    diff = abs(c - o)\n",
    "    o1, c1 = np.roll(o, 1), np.roll(c, 1)  #\n",
    "    min_oc = np.where(o < c, o, c)\n",
    "    max_oc = np.where(o > c, o, c)\n",
    "\n",
    "    pattern = np.where(\n",
    "      np.logical_and( min_oc - l > diff, h - max_oc < diff), 6,\n",
    "      np.where(np.logical_and( h - max_oc > diff, min_oc - l < diff),\n",
    "      4, np.where(np.logical_and(np.logical_and(c > o, c1 < o1), np.logical_and(c > o1, o < c1)),\n",
    "        5, np.where( min_oc - l > diff, 3,\n",
    "                      np.where(np.logical_and( h - max_oc > diff,\n",
    "                  min_oc - l < diff),\n",
    "                      2, np.where(np.logical_and(np.logical_and(c > o, c1 < o1), np.logical_and(c > o1, o < c1)),\n",
    "                      1, 0))))))\n",
    "    return pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def heikin_ashi_status( ha_open, ha_close):\n",
    "\n",
    "    candles = np.full_like(ha_close, '', dtype='U10')\n",
    "\n",
    "    for i in range(1, len(ha_close)):\n",
    "\n",
    "        if ha_close[i] > ha_open[i]: candles[i] = 2 #'Green'\n",
    "\n",
    "        elif ha_close[i] < ha_open[i]: candles[i] = 1 # 'Red'\n",
    "\n",
    "        else: candles[i] = 0 #'Neutral'\n",
    "\n",
    "    return candles\n",
    "\n",
    "def heikin_ashi_candles( open, high, low, close):\n",
    "\n",
    "    ha_low, ha_close = np.empty(len(close), dtype=np.float32), np.empty(len(close), dtype=np.float32)\n",
    "    ha_open, ha_high = np.empty(len(close), dtype=np.float32), np.empty(len(close), dtype=np.float32)\n",
    "\n",
    "    ha_open[0] = (open[0] + close[0]) / 2\n",
    "    ha_close[0] = (close[0] + open[0] + high[0] + low[0]) / 4\n",
    "\n",
    "    for i in range(1, len(close)):\n",
    "        ha_open[i] = (ha_open[i - 1] + ha_close[i - 1]) / 2\n",
    "        ha_close[i] = (open[i] + high[i] + low[i] + close[i]) / 4\n",
    "        ha_high[i] = max(high[i], ha_open[i], ha_close[i])\n",
    "        ha_low[i] = min(low[i], ha_open[i], ha_close[i])\n",
    "\n",
    "    return ha_open, ha_close, ha_high, ha_low\n",
    "\n",
    "\n",
    "def true_range( high, low, close):\n",
    "\n",
    "  close_shift = shift(close, 1)\n",
    "  high_low, high_close, low_close = np.array(high - low, dtype=np.float32), \\\n",
    "                                    np.array(abs(high - close_shift), dtype=np.float32), \\\n",
    "                                    np.array(abs(low - close_shift), dtype=np.float32)\n",
    "\n",
    "  true_range = np.max(np.hstack((high_low, high_close, low_close)).reshape(-1, 3), axis=1)\n",
    "\n",
    "  return true_range\n",
    "\n",
    "\n",
    "def shift(array, place):\n",
    "\n",
    "  array = np.array(array, dtype=np.float32)\n",
    "  shifted = np.roll(array, place)\n",
    "  shifted[0:place] = np.nan\n",
    "  shifted[np.isnan(shifted)] = np.nanmean(shifted)\n",
    "\n",
    "  return shifted\n",
    "\n",
    "\n",
    "def ma_based_supertrend_indicator( high, low, close, atr_length=10, atr_multiplier=3, ma_length=10):\n",
    "\n",
    "    # Calculate True Range and Smoothed ATR\n",
    "    tr = true_range(high, low, close)\n",
    "    atr = ema(tr, atr_length)\n",
    "\n",
    "    upper_band = (high + low) / 2 + (atr_multiplier * atr)\n",
    "    lower_band = (high + low) / 2 - (atr_multiplier * atr)\n",
    "\n",
    "    trend = np.zeros(len(atr))\n",
    "\n",
    "    # Calculate Moving Average\n",
    "    ema_values = ema(close, ma_length)\n",
    "\n",
    "    if ema_values[0] > lower_band[0]:\n",
    "        trend[0] = lower_band[0]\n",
    "    elif ema_values[0] < upper_band[0]:\n",
    "        trend[0] = upper_band[0]\n",
    "    else:\n",
    "        trend[0] = upper_band[0]\n",
    "\n",
    "    # Compute final upper and lower bands\n",
    "    for i in range(1, len(close)):\n",
    "        if ema_values[i] > trend[i - 1]:\n",
    "            trend[i] = max(trend[i - 1], lower_band[i])\n",
    "\n",
    "\n",
    "        elif ema_values[i] < trend[i - 1]:\n",
    "            trend[i] = min(trend[i - 1], upper_band[i])\n",
    "\n",
    "        else:\n",
    "            trend[i] = trend[i - 1]\n",
    "\n",
    "    status_value = np.where(ema_values > trend, 1.0, -1.0)\n",
    "\n",
    "    return trend, status_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def supertrend_status_crossover( status_value):\n",
    "\n",
    "\n",
    "    prev_status = np.roll(status_value, 1)\n",
    "    supertrend_status_crossover = np.where((prev_status < 0) & (status_value > 0), 1.0, np.where((prev_status > 0) & (status_value < 0), -1.0, 0))\n",
    "\n",
    "    return supertrend_status_crossover\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def supertrend_indicator(high, low, close, period, multiplier=1.0):\n",
    "\n",
    "    true_range_value = true_range(high, low, close)\n",
    "\n",
    "    smoothed_atr = ema(true_range_value, period)\n",
    "\n",
    "    upper_band = (high + low) / 2 + (multiplier * smoothed_atr)\n",
    "    lower_band = (high + low) / 2 - (multiplier * smoothed_atr)\n",
    "\n",
    "    supertrend = np.zeros(len(true_range_value))\n",
    "    trend = np.zeros(len(true_range_value))\n",
    "\n",
    "    if close[0] > upper_band[0]: trend[0] = upper_band[0]\n",
    "    elif close[0] < lower_band[0]: trend[0] = lower_band[0]\n",
    "    else:  trend[0] = upper_band[0]\n",
    "\n",
    "    for i in range(1, len(close)):\n",
    "\n",
    "        if close[i] > upper_band[i]: trend[i] = upper_band[i]\n",
    "        elif close[i] < lower_band[i]: trend[i] = lower_band[i]\n",
    "        else: trend[i] = trend[i - 1]\n",
    "\n",
    "    # Calculate Buy/Sell Signals using numpy where  # np.where( close > trend, '1 Buy', '-1 Sell')\n",
    "    status_value = np.where(close > trend, 1.0, -1.0)\n",
    "\n",
    "    return trend, status_value\n",
    "\n",
    "def supertrend_status_crossover(status_value):\n",
    "\n",
    "\n",
    "    prev_status = np.roll(status_value, 1)\n",
    "    supertrend_status_crossover = np.where((prev_status < 0) & (status_value > 0), 1.0, np.where((prev_status > 0) & (status_value < 0), -1.0, 0))\n",
    "\n",
    "    return supertrend_status_crossover\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc761857-01d2-47c4-8dd9-903c4a9d55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    \n",
    "    open_value, high, low, close = data['Open'] , data['High'], data['Low'], data['Close']\n",
    "\n",
    "    elastic_supertrend, es_status_value = ma_based_supertrend_indicator( high, low, close, atr_length=10, atr_multiplier=2.5, ma_length=10)\n",
    "\n",
    "    elastic_supertrend_crossover = supertrend_status_crossover(es_status_value)\n",
    "\n",
    "    supertrend, supertrend_status_value = supertrend_indicator(high, low, close, period= 10, multiplier=0.66)\n",
    "    supertrend_crossover = supertrend_status_crossover(supertrend_status_value)\n",
    "\n",
    "    candle_type_value  = candle_type(open_value, high, low, close)\n",
    "\n",
    "    ha_open, ha_close, ha_high, ha_low = heikin_ashi_candles(open_value, high, low, close)\n",
    "    heikin_ashi_candle = heikin_ashi_status(ha_open, ha_close)\n",
    "    \n",
    "    data['RSI']   = calculate_rsi(data['Close'])\n",
    "    data['STDEV'] = data['Close'].rolling(window=5).std()\n",
    "    data['Upper_Band'] = data['Close'].rolling(window=5).mean() + (data['Close'].rolling(window=5).std() * 2)\n",
    "    data['Lower_Band'] = data['Close'].rolling(window=5).mean() - (data['Close'].rolling(window=5).std() * 2)\n",
    "\n",
    "    data['candle_type'] = candle_type_value\n",
    "\n",
    "    data['supertrend']  = np.asarray(supertrend)\n",
    "    data['supertrend_status']  = np.asarray(supertrend_status_value)\n",
    "\n",
    "\n",
    "    data['supertrend_crossover'] = np.asarray(supertrend_crossover)\n",
    "    data['supertrend_value'] = np.asarray(supertrend)\n",
    "\n",
    "    data['elastic_supertrend'] = elastic_supertrend\n",
    "    data['elastic_supertrend_status'] = es_status_value\n",
    "    data['elastic_supertrend_cross'] = elastic_supertrend_crossover\n",
    "    \n",
    "    data['heikin_ashi'] = heikin_ashi_candle\n",
    "    \n",
    "    ema_3  = ema( (high+low/2), 3 )\n",
    "    ema_5  = ema( (high+low/2), 5 )\n",
    "    ema_7  = ema( (high+low/2), 7 )\n",
    "    ema_14 = ema( (high+low/2), 14)\n",
    "\n",
    "    # Add seasonality features\n",
    "    data['Day_of_Week'] = data.index.dayofweek + 1  # Monday=1, ..., Friday=5\n",
    "    data['Week_of_Month'] = (data.index.day - 1) // 7 + 1\n",
    "    data['Month'] = data.index.month\n",
    "\n",
    "    # Add numerical features\n",
    "    data['Prev_Close'] = data['Close'].shift(1)\n",
    "    data['Price_Range'] = data['High'] - data['Low']\n",
    "    data['Median_Price'] = (data['High'] + data['Low']) / 2\n",
    "    \n",
    "    \n",
    "    data['ema_3']  = ema_3\n",
    "    data['ema_5']  = ema_5\n",
    "    data['ema_7']  = ema_7\n",
    "    data['ema_14'] = ema_14\n",
    "\n",
    "    selected_columns = data[['ema_3', 'ema_5', 'ema_7' ]]\n",
    "\n",
    "    ema_mean = np.mean(selected_columns, axis=1)\n",
    "    data['ema_difference'] = pd.Series(np.subtract( ema_mean, ema_14 ))\n",
    "\n",
    "    data['daily_returns'] = data['Close'] - data['Close'].shift(1)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_features_lagged( lag_by= 9):\n",
    "    \n",
    "    # Create window-based features\n",
    "    window_size = lag_by\n",
    "\n",
    "    for i in range(1, window_size + 1):\n",
    "        data[f'Day_of_Week_T-{i}'] = data['Day_of_Week'].shift(i)\n",
    "        data[f'Week_of_Month_T-{i}'] = data['Week_of_Month'].shift(i)\n",
    "        data[f'Month_T-{i}'] = data['Month'].shift(i)\n",
    "        data[f'Close_T-{i}'] = data['Close'].shift(i)\n",
    "        data[f'Open_T-{i}'] = data['Open'].shift(i)\n",
    "        data[f'High_T-{i}'] = data['High'].shift(i)\n",
    "        data[f'Low_T-{i}']  = data['Low'].shift(i)\n",
    "        data[f'STDEV_T-{i}']   = data['STDEV'].shift(i)\n",
    "        data[f'RSI_T-{i}']  = data['RSI'].shift(i)\n",
    "        data[f'Price_Range_T-{i}']  = data['Price_Range'] .shift(i)\n",
    "        data[f'Median_Price_T-{i}']  = data['Median_Price'].shift(i)\n",
    "        data[f'Upper_Band_T-{i}'] = data['Upper_Band'].shift(i)\n",
    "        data[f'Lower_Band_T-{i}'] = data['Lower_Band'].shift(i)\n",
    "\n",
    "        data[f'heikin_ashi_T-{i}'] = data['heikin_ashi'].shift(i)\n",
    "        data[f'supertrend_T-{i}'] = data['supertrend'].shift(i)\n",
    "        data[f'supertrend_status_T-{i}'] = data['supertrend_status'].shift(i)\n",
    "        data[f'supertrend_crossover_T-{i}'] = data['supertrend_crossover'].shift(i)\n",
    "        data[f'elastic_supertrend_T-{i}']   = data['elastic_supertrend'].shift(i)\n",
    "        data[f'elastic_supertrend_status_T-{i}']   = data['elastic_supertrend_status'].shift(i)\n",
    "\n",
    "        data[f'elastic_supertrend_cross_T-{i}'] = data['elastic_supertrend_cross'].shift(i)\n",
    "        data[f'candle_type_T-{i}'] = data['candle_type'].shift(i)\n",
    "\n",
    "\n",
    "        data[f'ema_3_T-{i}']  = data['ema_3'].shift(i)\n",
    "        data[f'ema_5_T-{i}']  = data['ema_5'].shift(i)\n",
    "        data[f'ema_7_T-{i}']  = data['ema_7'].shift(i)\n",
    "        data[f'ema_14_T-{i}'] = data['ema_14'].shift(i)\n",
    "        data[f'ema_difference_T-{i}'] = data['ema_difference'].shift(i)\n",
    "        data[f'supertrend_value_T-{i}'] = data['supertrend_value'].shift(i)\n",
    "        data[f'daily_returns_T-{i}'] = data['daily_returns'].shift(i)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def return_df_day(daily_path):\n",
    "    \n",
    "    data = pd.read_csv(daily_path, sep='\\t')\n",
    "\n",
    "    # Rename columns as requested\n",
    "    data.rename(columns={\n",
    "    '<DATE>': 'Date',\n",
    "    '<OPEN>': 'Open',\n",
    "    '<HIGH>': 'High',\n",
    "    '<LOW>': 'Low',\n",
    "    '<CLOSE>': 'Close',\n",
    "    '<TICKVOL>': 'TickVol',\n",
    "    '<VOL>': 'Vol',\n",
    "    '<SPREAD>': 'Spread'\n",
    "    }, inplace=True)\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y.%m.%d')\n",
    "\n",
    "    # Set Date column as the index\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    data = data[['Open', 'High', 'Low', 'Close']]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e37fc9-7da2-4fa9-9ab0-b6404763b9bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Python Code To Make A Binary File From All Hour4 OHLC Currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b3bbb-9656-480e-8b07-c953fa6a0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "forex_pairs = [ 'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD', 'AUDUSD',\n",
    "    'CADCHF', 'CADJPY',\n",
    "    'CHFJPY', \n",
    "    'EURAUD', 'EURCAD', 'EURCHF', 'EURGBP', \n",
    "    'EURJPY', 'EURNZD', 'EURUSD',\n",
    "    'GBPAUD', 'GBPCAD', 'GBPCHF', \n",
    "    'GBPJPY', 'GBPUSD', 'GBPNZD',\n",
    "    'NZDCAD', 'NZDCHF', 'NZDJPY', 'NZDUSD',  \n",
    "    'USDCHF', 'USDCAD', 'USDJPY' ]\n",
    "\n",
    "\n",
    "currency_ohlc = []\n",
    "count = 0\n",
    "for pair in forex_pairs:\n",
    "    # daily_path = f'currency_data/{pair[:6]}_Daily.csv'\n",
    "    hour4_path = f'currency_data/{pair}_H4.csv'\n",
    "\n",
    "    data_h4     = rename_h4_df(hour4_path)\n",
    "\n",
    "    reshaped_h4 = combine_ohlc_into_single_day(data_h4)\n",
    "\n",
    "    features_h4 = add_ohlc_in_lagged(reshaped_h4,  lag_by= 7 )\n",
    "\n",
    "    features_h4 = add_features(features_h4,  lag_by= 7)\n",
    "    \n",
    "    currency_df = { \"hour4_features\": features_h4, \"symbol\": f\"{pair}\"  }\n",
    "    \n",
    "    currency_ohlc.append(currency_df)\n",
    "    count += 1\n",
    "    print(count)\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('hour4_less_features_data_lag_by_7.bin', 'wb') as file:\n",
    "    pickle.dump(currency_ohlc, file)\n",
    "    \n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open('hour4_less_features_data_lag_by_3.bin', 'wb') as file:\n",
    "#     pickle.dump(currency_ohlc, file)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cf20e-a73f-4f52-977b-196a2087fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WITH LESS FEATURES FOR HOUR 4 \n",
    "\n",
    "\n",
    "def rename_h4_df(hour4_path):\n",
    "\n",
    "    data_h4 = pd.read_csv(hour4_path, sep='\\t')\n",
    "\n",
    "    # Rename columns as requested\n",
    "    data_h4.rename(columns={\n",
    "        '<DATE>': 'Date',\n",
    "        '<OPEN>': 'Open_h4',\n",
    "        '<HIGH>': 'High_h4',\n",
    "        '<LOW>': 'Low_h4',\n",
    "        '<CLOSE>': 'Close_h4',\n",
    "        '<TICKVOL>': 'TickVol',\n",
    "        '<VOL>': 'Vol',\n",
    "        '<SPREAD>': 'Spread'\n",
    "    }, inplace=True)\n",
    "\n",
    "    data_h4['Date'] = pd.to_datetime(data_h4['Date'], format='%Y.%m.%d')\n",
    "\n",
    "    # Set Date column as the index\n",
    "    data_h4.set_index('Date', inplace=True)\n",
    "\n",
    "    data_h4 = data_h4[['Open_h4', 'High_h4', 'Low_h4', 'Close_h4']]\n",
    "\n",
    "    return data_h4\n",
    "\n",
    "\n",
    "def combine_ohlc_into_single_day(data_h4):\n",
    "    grouped = data_h4.groupby(data_h4.index.date)\n",
    "\n",
    "    # Create a new dataframe to store the result\n",
    "    reshaped_h4 = pd.DataFrame()\n",
    "\n",
    "    # Extract Open, High, Low, Close for each 4-hour period and reshape\n",
    "    for date, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        for i in range(0, len(group)):\n",
    "            if i == 0:\n",
    "                reshaped_h4.at[date, f'Open_h4_{i}'] = group.loc[i, 'Open_h4']\n",
    "                reshaped_h4.at[date, f'High_h4_{i}'] = group.loc[i, 'High_h4']\n",
    "                reshaped_h4.at[date, f'Low_h4_{i}'] = group.loc[i, 'Low_h4']\n",
    "                reshaped_h4.at[date, f'Close_h4_{i}'] = group.loc[i, 'Close_h4']\n",
    "            else:\n",
    "                reshaped_h4.at[date, f'Open_h4_{i}'] = group.loc[i, 'Open_h4']\n",
    "                reshaped_h4.at[date, f'High_h4_{i}'] = group.loc[i, 'High_h4']\n",
    "                reshaped_h4.at[date, f'Low_h4_{i}'] = group.loc[i, 'Low_h4']\n",
    "                reshaped_h4.at[date, f'Close_h4_{i}'] = group.loc[i, 'Close_h4']\n",
    "\n",
    "    return reshaped_h4\n",
    "    \n",
    "\n",
    "def add_ohlc_in_lagged(reshaped_h4, lag_by= 7):\n",
    "    \n",
    "    features_h4 = pd.DataFrame()\n",
    "    for candles in range(0, 6): # 0 --> 5 all 6 candles\n",
    "        for day in range(1, lag_by + 1): # last 3 days = 6 * 3 = Last H4 18 candles\n",
    " \n",
    "            # new name will be candle number and day shifted from\n",
    "            features_h4[f'Close_h4_{candles}_T-{day}'] = reshaped_h4[f'Close_h4_{candles}'].shift(day)\n",
    "            features_h4[f'High_h4_{candles}_T-{day}']  = reshaped_h4[f'High_h4_{candles}'].shift(day)\n",
    "            features_h4[f'Open_h4_{candles}_T-{day}']  = reshaped_h4[f'Open_h4_{candles}'].shift(day)\n",
    "            features_h4[f'Low_h4_{candles}_T-{day}']  = reshaped_h4[f'Low_h4_{candles}'].shift(day)\n",
    "    \n",
    "    return  features_h4\n",
    "\n",
    "\n",
    "def add_features(features_h4, lag_by= 7):\n",
    "    \n",
    "    # features_h4.fillna(0.0, inplace= True)\n",
    "    features_h4 = features_h4.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    \n",
    "    for candles in range(0, 6): # 0 --> 5 all 6 candles\n",
    "        for day in range(1, lag_by + 1): # last 3 days = 6 * 3 = Last H4 18 candles\n",
    "            pass \n",
    "            # ema3_column  = f'ema3_h4_{candles}_T-{day}'\n",
    "            # ema5_column  = f'ema5_h4_{candles}_T-{day}'\n",
    "            # ema7_column  = f'ema7_h4_{candles}_T-{day}'\n",
    "            # ema14_column = f'ema14_h4_{candles}_T-{day}'\n",
    "\n",
    "            # features_h4[f'Low_h4_{candles}_T-{day}'].fillna(features_h4[f'Low_h4_{candles}_T-{day}'].mean(), inplace=True)\n",
    "            # features_h4[f'High_h4_{candles}_T-{day}'].fillna(features_h4[f'High_h4_{candles}_T-{day}'].mean(), inplace=True)\n",
    "            # features_h4[f'Close_h4_{candles}_T-{day}'].fillna(features_h4[f'Close_h4_{candles}_T-{day}'].mean(), inplace=True)\n",
    "            \n",
    "            open_value  = features_h4[f'Open_h4_{candles}_T-{day}'].values\n",
    "            close_value = features_h4[f'Close_h4_{candles}_T-{day}'].values\n",
    "            high_value = features_h4[f'High_h4_{candles}_T-{day}'].values\n",
    "            low_value  = features_h4[f'Low_h4_{candles}_T-{day}'].values\n",
    "\n",
    "            open_val  = features_h4[f'Open_h4_{candles}_T-{day}']\n",
    "            close_val = features_h4[f'Close_h4_{candles}_T-{day}']\n",
    "            high_val = features_h4[f'High_h4_{candles}_T-{day}']\n",
    "            low_val  = features_h4[f'Low_h4_{candles}_T-{day}']\n",
    "\n",
    "            hlc = ( features_h4[f'High_h4_{candles}_T-{day}'] + features_h4[f'Low_h4_{candles}_T-{day}'] + features_h4[f'Close_h4_{candles}_T-{day}']) / 3 \n",
    "\n",
    "            features_h4[f'true_range_h4_{candles}_T-{day}'] = pd.Series(high_value- low_value)\n",
    "\n",
    "            features_h4[f'median_h4_{candles}_T-{day}'] = pd.Series( (high_value + low_value) / 2 )\n",
    "            \n",
    "#             features_h4[f'RSI_slow_{candles}_T-{day}']   = calculate_rsi(close_val, period=21)\n",
    "#             features_h4[f'RSI_fast_{candles}_T-{day}']   = calculate_rsi(close_val, period=9)\n",
    "            \n",
    "            features_h4[f'stdev_slow_{candles}_T-{day}'] = close_val.rolling(window=18).std()\n",
    "            features_h4[f'stdev_fast_{candles}_T-{day}'] = close_val.rolling(window=9).std()\n",
    "    \n",
    "#             features_h4[f'Upper_Band_slow_{candles}_T-{day}'] = close_val.rolling(window=18).mean() + (close_val.rolling(window=5).std() * 2)\n",
    "#             features_h4[f'Lower_Band_slow_{candles}_T-{day}'] = close_val.rolling(window=18).mean() - (close_val.rolling(window=5).std() * 2)\n",
    "\n",
    "#             features_h4[f'Upper_Band_fast_{candles}_T-{day}'] = close_val.rolling(window=9).mean() + (close_val.rolling(window=5).std() * 2)\n",
    "#             features_h4[f'Lower_Band_fast_{candles}_T-{day}'] = close_val.rolling(window=9).mean() - (close_val.rolling(window=5).std() * 2)\n",
    "\n",
    "#             ema3_value  = hlc.ewm(span=3, adjust=False).mean()\n",
    "#             ema5_value  = hlc.ewm(span=5, adjust=False).mean()\n",
    "#             ema7_value  = hlc.ewm(span=7, adjust=False).mean()\n",
    "#             ema14_value = hlc.ewm(span=14, adjust=False).mean()\n",
    "\n",
    "            highest_high = features_h4[f'High_h4_{candles}_T-{day}'].rolling(window=9).max()\n",
    "            lowest_low   = features_h4[f'Low_h4_{candles}_T-{day}'].rolling(window=9).min()\n",
    "\n",
    "            # Calculate the relative range\n",
    "            features_h4[f'relative_range_h4_{candles}_T-{day}'] = close_value - ( ( highest_high + lowest_low ) / 2)\n",
    "\n",
    "\n",
    "            candle_type_value  = candle_type(open_value, high_value, low_value, close_value)\n",
    "\n",
    "\n",
    "#             elastic_supertrend, es_status_value = ma_based_supertrend_indicator( high_value, low_value, close_value, atr_length=9, atr_multiplier=2.5, ma_length=9)\n",
    "\n",
    "#             elastic_supertrend_crossover = supertrend_status_crossover(es_status_value)\n",
    "\n",
    "#             supertrend, supertrend_status_value = supertrend_indicator(high_value, low_value, close_value, period= 9, multiplier=0.66)\n",
    "#             supertrend_crossover = supertrend_status_crossover(supertrend_status_value)\n",
    "\n",
    "#             features_h4[f'supertrend_h4_{candles}_T-{day}'] = supertrend\n",
    "\n",
    "#             features_h4[f'supertrend_status_h4_{candles}_T-{day}'] = supertrend_status_value\n",
    "\n",
    "#             features_h4[f'supertrend_crossover_h4_{candles}_T-{day}'] = supertrend_crossover\n",
    "\n",
    "\n",
    "#             features_h4[f'es_supertrend_h4_{candles}_T-{day}'] = elastic_supertrend\n",
    "\n",
    "#             features_h4[f'es_supertrend_crossover_h4_{candles}_T-{day}'] = elastic_supertrend_crossover\n",
    "\n",
    "#             features_h4[f'es_supertrend_status_h4_{candles}_T-{day}'] = es_status_value\n",
    "\n",
    "            \n",
    "            features_h4[f'candle_type_h4_{candles}_T-{day}'] = candle_type_value\n",
    "\n",
    "            # features_h4[f'smi_direct_h4_{candles}_T-{day}']  = smi_fast_direction\n",
    "\n",
    "            # features_h4[f'smi_h4_{candles}_T-{day}'] = smi_fast\n",
    "\n",
    "#             features_h4[f'ema3_h4_{candles}_T-{day}']  = pd.Series(ema3_value)\n",
    "\n",
    "#             features_h4[f'ema5_h4_{candles}_T-{day}']  = pd.Series(ema5_value)\n",
    "\n",
    "#             features_h4[f'ema7_h4_{candles}_T-{day}']  = pd.Series(ema7_value)\n",
    "\n",
    "#             features_h4[f'ema14_h4_{candles}_T-{day}'] = pd.Series(ema14_value)\n",
    "\n",
    "            ha_open, ha_close, ha_high, ha_low = heikin_ashi_candles(open_value, high_value, low_value, close_value)\n",
    "            heikin_ashi_candle = heikin_ashi_status(ha_open, ha_close)\n",
    "            features_h4[f'heikin_ashi_{candles}_T-{day}'] = heikin_ashi_candle\n",
    "\n",
    "#             selected_columns = features_h4[[ema3_column, ema5_column, ema7_column ]]\n",
    "\n",
    "#             ema_mean = np.mean(selected_columns, axis=1)\n",
    "#             features_h4[f'ema_difference_{candles}_T-{day}'] = pd.Series(np.subtract( ema_mean, ema14_value ))\n",
    "\n",
    "        \n",
    "    return  features_h4\n",
    "\n",
    "\n",
    "\n",
    "def ema( price, period):\n",
    "\n",
    "  price = np.array(price)\n",
    "  alpha = 2 / (period + 1.0)\n",
    "  alpha_reverse = 1 - alpha\n",
    "  data_length = len(price)\n",
    "\n",
    "  power_factors = alpha_reverse ** (np.arange(data_length + 1))\n",
    "  initial_offset = price[0] * power_factors[1:]\n",
    "\n",
    "  scale_factors = 1 / power_factors[:-1]\n",
    "\n",
    "  weight_factor = alpha * alpha_reverse ** (data_length - 1)\n",
    "\n",
    "  weighted_price_data = price * weight_factor * scale_factors\n",
    "  cumulative_sums = weighted_price_data.cumsum()\n",
    "  ema_values = initial_offset + cumulative_sums * scale_factors[::-1]\n",
    "\n",
    "  return ema_values\n",
    "    \n",
    "\n",
    "\n",
    "# Calculate RSI\n",
    "def calculate_rsi(series, period=5):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "    \n",
    "def moving_max(array, window_size):\n",
    "   \n",
    "    rolling_max = np.full(array.shape, 0.0)\n",
    "    \n",
    "    for i in range(len(array) - window_size + 1):\n",
    "        window_values = array[i:i + window_size]\n",
    "        rolling_max[i + window_size - 1] = np.max(window_values)\n",
    "        \n",
    "    rolling_max[np.isnan(rolling_max)] = np.nanmean(rolling_max)\n",
    "    return rolling_max    \n",
    "    \n",
    "\n",
    "def moving_min(array, window_size):\n",
    "\n",
    "    rolling_min = np.full(array.shape, 0.0)\n",
    "    for i in range(len(array) - window_size + 1):\n",
    "        window_values = array[i:i + window_size]\n",
    "        rolling_min[i + window_size - 1] = np.min(window_values)\n",
    "        \n",
    "    rolling_min[np.isnan(rolling_min)] = np.nanmean(rolling_min)\n",
    "    return rolling_min\n",
    "\n",
    "\n",
    "\n",
    "def true_range( high, low, close):\n",
    "\n",
    "  close_shift = shift(close, 1)\n",
    "  high_low, high_close, low_close = np.array(high - low, dtype=np.float32), \\\n",
    "                                    np.array(abs(high - close_shift), dtype=np.float32), \\\n",
    "                                    np.array(abs(low - close_shift), dtype=np.float32)\n",
    "\n",
    "  true_range = np.max(np.hstack((high_low, high_close, low_close)).reshape(-1, 3), axis=1)\n",
    "\n",
    "  return true_range\n",
    "\n",
    "\n",
    "def shift(array, place):\n",
    "\n",
    "  array = np.array(array, dtype=np.float32)\n",
    "  shifted = np.roll(array, place)\n",
    "  shifted[0:place] = np.nan\n",
    "  shifted[np.isnan(shifted)] = np.nanmean(shifted)\n",
    "\n",
    "  return shifted\n",
    "\n",
    "\n",
    "def ma_based_supertrend_indicator( high, low, close, atr_length=10, atr_multiplier=3, ma_length=10):\n",
    "\n",
    "    # Calculate True Range and Smoothed ATR\n",
    "    tr = true_range(high, low, close)\n",
    "    atr = ema(tr, atr_length)\n",
    "\n",
    "    upper_band = (high + low) / 2 + (atr_multiplier * atr)\n",
    "    lower_band = (high + low) / 2 - (atr_multiplier * atr)\n",
    "\n",
    "    trend = np.zeros(len(atr))\n",
    "\n",
    "    # Calculate Moving Average\n",
    "    ema_values = ema(close, ma_length)\n",
    "\n",
    "    if ema_values[0] > lower_band[0]:\n",
    "        trend[0] = lower_band[0]\n",
    "    elif ema_values[0] < upper_band[0]:\n",
    "        trend[0] = upper_band[0]\n",
    "    else:\n",
    "        trend[0] = upper_band[0]\n",
    "\n",
    "    # Compute final upper and lower bands\n",
    "    for i in range(1, len(close)):\n",
    "        if ema_values[i] > trend[i - 1]:\n",
    "            trend[i] = max(trend[i - 1], lower_band[i])\n",
    "\n",
    "\n",
    "        elif ema_values[i] < trend[i - 1]:\n",
    "            trend[i] = min(trend[i - 1], upper_band[i])\n",
    "\n",
    "        else:\n",
    "            trend[i] = trend[i - 1]\n",
    "\n",
    "    status_value = np.where(ema_values > trend, 1.0, -1.0)\n",
    "\n",
    "    return trend, status_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def supertrend_status_crossover( status_value):\n",
    "\n",
    "\n",
    "    prev_status = np.roll(status_value, 1)\n",
    "    supertrend_status_crossover = np.where((prev_status < 0) & (status_value > 0), 1.0, np.where((prev_status > 0) & (status_value < 0), -1.0, 0))\n",
    "\n",
    "    return supertrend_status_crossover\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def supertrend_indicator(high, low, close, period, multiplier=1.0):\n",
    "\n",
    "    true_range_value = true_range(high, low, close)\n",
    "\n",
    "    smoothed_atr = ema(true_range_value, period)\n",
    "\n",
    "    upper_band = (high + low) / 2 + (multiplier * smoothed_atr)\n",
    "    lower_band = (high + low) / 2 - (multiplier * smoothed_atr)\n",
    "\n",
    "    supertrend = np.zeros(len(true_range_value))\n",
    "    trend = np.zeros(len(true_range_value))\n",
    "\n",
    "    if close[0] > upper_band[0]: trend[0] = upper_band[0]\n",
    "    elif close[0] < lower_band[0]: trend[0] = lower_band[0]\n",
    "    else:  trend[0] = upper_band[0]\n",
    "\n",
    "    for i in range(1, len(close)):\n",
    "\n",
    "        if close[i] > upper_band[i]: trend[i] = upper_band[i]\n",
    "        elif close[i] < lower_band[i]: trend[i] = lower_band[i]\n",
    "        else: trend[i] = trend[i - 1]\n",
    "\n",
    "    # Calculate Buy/Sell Signals using numpy where  # np.where( close > trend, '1 Buy', '-1 Sell')\n",
    "    status_value = np.where(close > trend, 1.0, -1.0)\n",
    "\n",
    "    return trend, status_value\n",
    "\n",
    "def supertrend_status_crossover(status_value):\n",
    "\n",
    "\n",
    "    prev_status = np.roll(status_value, 1)\n",
    "    supertrend_status_crossover = np.where((prev_status < 0) & (status_value > 0), 1.0, np.where((prev_status > 0) & (status_value < 0), -1.0, 0))\n",
    "\n",
    "    return supertrend_status_crossover\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def direction_crossover_signal_line(signal, signal_ema):\n",
    "\n",
    "    direction = np.where(signal - signal_ema > 0, 1, -1)\n",
    "    prev_direction = np.roll(direction, 1)\n",
    "    crossover = np.where((prev_direction < 0) & (direction > 0), 1,\n",
    "                          np.where((prev_direction > 0) & (direction < 0), -1, 0))\n",
    "\n",
    "    return direction, crossover\n",
    "\n",
    "\n",
    "def stochastic_momentum_index(high, low, close, period=20, ema_period=5):\n",
    "    # Compute highest high and lowest low over the period\n",
    "    highest_high = high.rolling(window=period).max()\n",
    "    lowest_low = low.rolling(window=period).min()\n",
    "\n",
    "    # Compute relative range\n",
    "    relative_range = close - ((highest_high + lowest_low) / 2)\n",
    "\n",
    "    # Compute highest-lowest range\n",
    "    highest_lowest_range = highest_high - lowest_low\n",
    "\n",
    "    # Smooth relative range and highest-lowest range\n",
    "    relative_range_smoothed = relative_range.ewm(span=ema_period, adjust=False).mean().ewm(span=ema_period, adjust=False).mean()\n",
    "    highest_lowest_range_smoothed = highest_lowest_range.ewm(span=ema_period, adjust=False).mean().ewm(span=ema_period, adjust=False).mean()\n",
    "\n",
    "    # Calculate SMI\n",
    "    smi = (relative_range_smoothed / (highest_lowest_range_smoothed / 2)) * 100\n",
    "    smi[smi == np.inf] = 0  # Replace infinite values with 0\n",
    "    smi_ema = smi.ewm(span=ema_period, adjust=False).mean()\n",
    "\n",
    "    return smi, smi_ema\n",
    "\n",
    "# def stochastic_momentum_index(high, low, close, period=20, ema_period=5):\n",
    "\n",
    "#     lengthD = ema_period\n",
    "#     lowest_low   = moving_min(low, period)\n",
    "#     highest_high = moving_max(high, period)\n",
    "#     relative_range = close - ((highest_high + lowest_low) / 2)\n",
    "#     highest_lowest_range = highest_high - lowest_low\n",
    "\n",
    "#     relative_range_smoothed = ema(ema(relative_range, ema_period), ema_period)\n",
    "#     highest_lowest_range_smoothed = ema(ema(highest_lowest_range, ema_period), ema_period)\n",
    "\n",
    "#     smi = [(relative_range_smoothed[i] / (highest_lowest_range_smoothed[i] / 2)) * 100 if\n",
    "#             highest_lowest_range_smoothed[i] != 0 else 0.0\n",
    "#             for i in range(len(relative_range_smoothed))]\n",
    "\n",
    "#     smi_ema = ema(smi, ema_period)\n",
    "\n",
    "#     return smi, smi_ema\n",
    "\n",
    "\n",
    "def candle_type(o, h, l, c):\n",
    "\n",
    "    diff = abs(c - o)\n",
    "    o1, c1 = np.roll(o, 1), np.roll(c, 1)  #\n",
    "    min_oc = np.where(o < c, o, c)\n",
    "    max_oc = np.where(o > c, o, c)\n",
    "\n",
    "    pattern = np.where(\n",
    "      np.logical_and( min_oc - l > diff, h - max_oc < diff), 6,\n",
    "      np.where(np.logical_and( h - max_oc > diff, min_oc - l < diff),\n",
    "      4, np.where(np.logical_and(np.logical_and(c > o, c1 < o1), np.logical_and(c > o1, o < c1)),\n",
    "        5, np.where( min_oc - l > diff, 3,\n",
    "                      np.where(np.logical_and( h - max_oc > diff,\n",
    "                  min_oc - l < diff),\n",
    "                      2, np.where(np.logical_and(np.logical_and(c > o, c1 < o1), np.logical_and(c > o1, o < c1)),\n",
    "                      1, 0))))))\n",
    "    return pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def heikin_ashi_status( ha_open, ha_close):\n",
    "\n",
    "    candles = np.full_like(ha_close, '', dtype='U10')\n",
    "\n",
    "    for i in range(1, len(ha_close)):\n",
    "\n",
    "        if ha_close[i] > ha_open[i]: candles[i] = 2 #'Green'\n",
    "\n",
    "        elif ha_close[i] < ha_open[i]: candles[i] = 1 # 'Red'\n",
    "\n",
    "        else: candles[i] = 0 #'Neutral'\n",
    "\n",
    "    return candles\n",
    "\n",
    "def heikin_ashi_candles( open, high, low, close):\n",
    "\n",
    "    ha_low, ha_close = np.empty(len(close), dtype=np.float32), np.empty(len(close), dtype=np.float32)\n",
    "    ha_open, ha_high = np.empty(len(close), dtype=np.float32), np.empty(len(close), dtype=np.float32)\n",
    "\n",
    "    ha_open[0] = (open[0] + close[0]) / 2\n",
    "    ha_close[0] = (close[0] + open[0] + high[0] + low[0]) / 4\n",
    "\n",
    "    for i in range(1, len(close)):\n",
    "        ha_open[i] = (ha_open[i - 1] + ha_close[i - 1]) / 2\n",
    "        ha_close[i] = (open[i] + high[i] + low[i] + close[i]) / 4\n",
    "        ha_high[i] = max(high[i], ha_open[i], ha_close[i])\n",
    "        ha_low[i] = min(low[i], ha_open[i], ha_close[i])\n",
    "\n",
    "    return ha_open, ha_close, ha_high, ha_low\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
